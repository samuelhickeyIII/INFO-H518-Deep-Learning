{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data description\n",
    "This dataset has three columns - label (party name), twitter handle, tweet text\n",
    "\n",
    "\n",
    "#### Problem Description:\n",
    "\n",
    "Design a feed forward deep neural network to predict the political party using the pytorch or tensorflow. \n",
    "Build two models\n",
    "\n",
    "1. Without using the handle\n",
    "\n",
    "2. Using the handle\n",
    "\n",
    "\n",
    "#### Deliverables\n",
    "\n",
    "- Report the performance on the test set.\n",
    "\n",
    "- Try multiple models and with different hyperparameters. Present the results of each model on the test set. No need to create a dev set.\n",
    "\n",
    "- Experiment with:\n",
    "    -L2 and dropout regularization techniques\n",
    "    -SGD, RMSProp and Adamp optimization techniques\n",
    "\n",
    "\n",
    "\n",
    "- Creating a fixed-sized vocabulary: Give a unique id to each word in your selected vocabulary and use it as the input to the network\n",
    "\n",
    "    - Option 1: Feedforward networks can only handle fixed-sized inputs. You can choose to have a fixed-sized K words from the tweet text (e.g. the first K word, randomly selected K word etc.). K can be a hyperparameter. \n",
    "\n",
    "    - Option 2: you can choose top N (e.g. N=1000) frequent words from the dataset and use an N-sized input layer. If a word is present in a tweet, pass the id, 0 otherwise\n",
    "    \n",
    "    -  Clearly state your design choices and assumptions. Think about the pros and cons of each option.\n",
    "\n",
    " \n",
    "\n",
    "<b> Tabulate your results, either at the end of the code file or in the text box on the submission page. The final result should have:</b>\n",
    "\n",
    "1. Experiment description\n",
    "\n",
    "2. Hyperparameter used and their values\n",
    "\n",
    "3. Performance on the test set\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, losses\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import text\n",
    "from keras.utils import np_utils\n",
    "from random import randrange\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "PATH = r\"C:\\Users\\samue\\Documents\\Applied Data Science\\INFO-H518 Deep Learning\\Assignments\\A3\\Input\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the data\n",
    "train = pd.read_pickle(PATH + r'\\train_tokenized.pickle').dropna().sample(frac=1)\n",
    "train_vocab = pd.read_csv(PATH + r'\\train_vocab_frequency.csv', index_col=0).dropna()\n",
    "test = pd.read_pickle(PATH + r'\\test_tokenized.pickle').dropna().sample(frac=1)\n",
    "test_vocab = pd.read_csv(PATH + r'\\test_vocab_frequency.csv', index_col=0).dropna()\n",
    "vocab = train_vocab.append(test_vocab).reset_index()\n",
    "\n",
    "# Change Party/Handle to Categoricals\n",
    "train.Party = pd.Categorical(train.Party)\n",
    "train['Party'] = train.Party.cat.codes\n",
    "\n",
    "train_hands = train.Handle.unique().rename(columns={'Handle':'Terms'})\n",
    "train.Handle = pd.Categorical(train.Handle)\n",
    "train['Handle'] = train.Handle.cat.codes\n",
    "\n",
    "test.Party = pd.Categorical(test.Party)\n",
    "test['Party'] = test.Party.cat.codes\n",
    "\n",
    "test\n",
    "test.Handle = pd.Categorical(test.Handle)\n",
    "test['Handle'] = test.Handle.cat.codes\n",
    "\n",
    "# Remove 10% of the vocabulary (specifcially infrequent terms)\n",
    "vocab_cut = int(train_vocab.shape[0] * (10 / 100))\n",
    "vocab = vocab.iloc[vocab_cut:].drop_duplicates('Terms').reset_index().drop(columns=['index', 'level_0'])\n",
    "vocab = {v:k for k, v in vocab.to_dict()['Terms'].items()}\n",
    "\n",
    "# Provide each term a unique id\n",
    "train_words_i = {v:vocab[v] for v in list(train_vocab.to_dict()['Terms'].values()) if v in vocab.keys()}\n",
    "test_words_i = {v:vocab[v] for v in list(test_vocab.to_dict()['Terms'].values()) if v in vocab.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model #1: Select the first K words from the Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_k_words(words_i:dict, data: pd.DataFrame, k: int):\n",
    "    for i, row in data.iterrows():\n",
    "        terms, token_count = [], len(row['Tokens'])\n",
    "        if token_count > 0:\n",
    "            j = 0\n",
    "            while len(terms) < k:\n",
    "                if row['Tokens'][j] in words_i.keys():\n",
    "                    terms.append(words_i[row['Tokens'][j]])\n",
    "                else:\n",
    "                    terms.append(np.float32(0))\n",
    "                j = j+1 if j+1 < token_count else 0\n",
    "        else:\n",
    "            continue\n",
    "        x = np.array(terms, dtype=np.float32)\n",
    "        y = row['Party']\n",
    "        yield (x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M1 | Build the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Build Training Data...\n",
      "[####################]\n",
      " Build Testing Data...\n",
      "[####################]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(20,), dtype=float32, numpy=\n",
       " array([63422., 63869., 63120., 63874., 63647., 61277., 63746., 63703.,\n",
       "        63784., 63871., 63863., 63725., 63442., 63817., 63858., 63822.,\n",
       "        62282., 63854., 63422., 63869.], dtype=float32)>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters\n",
    "epochs = 5\n",
    "embedding_dim = 32\n",
    "k = 20\n",
    "\n",
    "# Train\n",
    "print(\" Build Training Data...\\n[#\", end='')\n",
    "train_data, train_labels = [], []\n",
    "for x, y in first_k_words(train_words_i, train, k):\n",
    "    train_data.append(x)\n",
    "    train_labels.append(y)\n",
    "    if len(train_labels) % int(train.shape[0]*.1) == 0: \n",
    "        print(f\"##\", end='')\n",
    "train_labels = tf.convert_to_tensor(train_labels, dtype=tf.float32)\n",
    "train_data = tf.cast(train_data, dtype=tf.float32)\n",
    "print(\"#]\")\n",
    "\n",
    "# Validation\n",
    "split = round(train_data.shape[0]*.8)\n",
    "valid_data, valid_labels = train_data[split:], train_labels[split:]\n",
    "train_data, train_labels = train_data[:split], train_labels[:split]\n",
    "\n",
    "# Test\n",
    "print(\" Build Testing Data...\\n[#\", end='')\n",
    "test_data, test_labels = [], []\n",
    "for x, y in first_k_words(test_words_i, test, k):\n",
    "    test_data.append(x)\n",
    "    test_labels.append(y)\n",
    "    if len(test_labels) % int(test.shape[0]*.1) == 0: \n",
    "        print(f\"##\", end='')\n",
    "    \n",
    "test_labels = tf.convert_to_tensor(test_labels, dtype=tf.float32)\n",
    "test_data = tf.cast(test_data, dtype=tf.float32)\n",
    "print(\"#]\")\n",
    "\n",
    "(train_data[0], train_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M1 | Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure\n",
    "k_words = Sequential([\n",
    "    layers.Embedding(len(vocab)+1, embedding_dim),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.GlobalAveragePooling1D(),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "k_words.compile(\n",
    "    loss=losses.BinaryCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.Adam(\n",
    "        learning_rate=0.002,\n",
    "        beta_1=0.8,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-07,\n",
    "        amsgrad=False,\n",
    "        name='Adam'\n",
    "    ),\n",
    "    metrics=tf.metrics.BinaryAccuracy(threshold=0.0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M1 | Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1812/1812 [==============================] - 11s 5ms/step - loss: 0.5278 - binary_accuracy: 0.7327 - val_loss: 0.4494 - val_binary_accuracy: 0.7801\n",
      "Epoch 2/5\n",
      "1812/1812 [==============================] - 9s 5ms/step - loss: 0.3478 - binary_accuracy: 0.8485 - val_loss: 0.4361 - val_binary_accuracy: 0.7949\n",
      "Epoch 3/5\n",
      "1812/1812 [==============================] - 9s 5ms/step - loss: 0.2674 - binary_accuracy: 0.8884 - val_loss: 0.4592 - val_binary_accuracy: 0.7975\n",
      "Epoch 4/5\n",
      "1812/1812 [==============================] - 9s 5ms/step - loss: 0.2213 - binary_accuracy: 0.9098 - val_loss: 0.4913 - val_binary_accuracy: 0.7964\n",
      "Epoch 5/5\n",
      "1812/1812 [==============================] - 9s 5ms/step - loss: 0.1902 - binary_accuracy: 0.9238 - val_loss: 0.5304 - val_binary_accuracy: 0.7954\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\samue\\Documents\\Applied Data Science\\INFO-H518 Deep Learning\\Assignments\\A3\\Models\\M1\\assets\n"
     ]
    }
   ],
   "source": [
    "history = k_words.fit(\n",
    "    train_data,\n",
    "    train_labels,\n",
    "    validation_data=(valid_data, valid_labels),\n",
    "    epochs=epochs\n",
    ")\n",
    "k_words.save(r'C:\\Users\\samue\\Documents\\Applied Data Science\\INFO-H518 Deep Learning\\Assignments\\A3\\Models\\M1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M1 | Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "428/428 [==============================] - 1s 2ms/step - loss: 0.5387 - binary_accuracy: 0.7905\n",
      "Loss: 0.5386927723884583 \n",
      "Accuracy: 0.7904887199401855\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = k_words.evaluate(test_data, test_labels)\n",
    "print(f\"Loss: {loss} \\nAccuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use FIRST K words from a tweet to predict the tweeter's political party\n",
    "(fill feature vector to length K if len(feature vector) < K)\n",
    "\n",
    "Epoch = 5 \n",
    "\n",
    "K = 20 \n",
    "\n",
    "Embedding_dim = 32 \n",
    "\n",
    "Optimizer = 'adam', lr = .002, b1 = .8 \n",
    "\n",
    "Loss = .5387\n",
    "\n",
    "Accuracy = 0.7904"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model #2: Select K-random words from the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_k_words(words_i:dict, data: pd.DataFrame, k: int):\n",
    "    for i, row in data.iterrows():\n",
    "        terms = []\n",
    "        for _ in range(k):\n",
    "            if len(row['Tokens']) > 0:\n",
    "                token = random.choice(row['Tokens'])\n",
    "                val = words_i[token] \\\n",
    "                    if token in words_i.keys() \\\n",
    "                    else np.float32(0)\n",
    "                terms.append(val)\n",
    "            else:\n",
    "                terms = [np.float32(0) for _ in range(k)]\n",
    "                break\n",
    "        x = np.array(terms, dtype=np.float32)\n",
    "        y = row['Party']\n",
    "        yield (x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M2 | Build the data (let params remain the same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(20,), dtype=float32, numpy=\n",
       " array([62282., 63822., 63817., 63854., 63647., 63869., 63422., 63422.,\n",
       "        63746., 63442., 63442., 63874., 63863., 63422., 61277., 63869.,\n",
       "        63863., 61277., 63120., 62282.], dtype=float32)>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters\n",
    "epochs = 5\n",
    "embedding_dim = 32\n",
    "k = 20\n",
    "\n",
    "# Train\n",
    "train_data, train_labels = [], []\n",
    "for x, y in random_k_words(train_words_i, train, k):\n",
    "    train_data.append(x)\n",
    "    train_labels.append(y)\n",
    "\n",
    "train_labels = tf.convert_to_tensor(train_labels, dtype=tf.float32)\n",
    "train_data = tf.cast(train_data, dtype=tf.float32)\n",
    "\n",
    "# Validation\n",
    "split = round(train_data.shape[0]*.8)\n",
    "valid_data, valid_labels = train_data[split:], train_labels[split:]\n",
    "train_data, train_labels = train_data[:split], train_labels[:split]\n",
    "\n",
    "# Test\n",
    "test_data, test_labels = [], []\n",
    "for x, y in random_k_words(test_words_i, test, k):\n",
    "    test_data.append(x)\n",
    "    test_labels.append(y)\n",
    "    \n",
    "test_labels = tf.convert_to_tensor(test_labels, dtype=tf.float32)\n",
    "test_data = tf.cast(test_data, dtype=tf.float32)\n",
    "(train_data[0], train_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M2 | Leave the structure the same to allow for comparison of k-selection methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure\n",
    "kr_words = Sequential([\n",
    "    layers.Embedding(len(vocab)+1, embedding_dim),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.GlobalAveragePooling1D(),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "kr_words.compile(\n",
    "    loss=losses.BinaryCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.Adam(\n",
    "        learning_rate=0.002,\n",
    "        beta_1=0.8,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-07,\n",
    "        amsgrad=False,\n",
    "        name='Adam'\n",
    "    ),\n",
    "    metrics=tf.metrics.BinaryAccuracy(threshold=0.0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M2 | Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1819/1819 [==============================] - 9s 5ms/step - loss: 0.5796 - binary_accuracy: 0.6874 - val_loss: 0.5217 - val_binary_accuracy: 0.7295\n",
      "Epoch 2/5\n",
      "1819/1819 [==============================] - 9s 5ms/step - loss: 0.4253 - binary_accuracy: 0.8024 - val_loss: 0.5200 - val_binary_accuracy: 0.7406\n",
      "Epoch 3/5\n",
      "1819/1819 [==============================] - 8s 4ms/step - loss: 0.3480 - binary_accuracy: 0.8449 - val_loss: 0.5485 - val_binary_accuracy: 0.7437\n",
      "Epoch 4/5\n",
      "1819/1819 [==============================] - 8s 5ms/step - loss: 0.3012 - binary_accuracy: 0.8683 - val_loss: 0.5854 - val_binary_accuracy: 0.7459\n",
      "Epoch 5/5\n",
      "1819/1819 [==============================] - 9s 5ms/step - loss: 0.2690 - binary_accuracy: 0.8814 - val_loss: 0.6300 - val_binary_accuracy: 0.7407\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\samue\\Documents\\Applied Data Science\\INFO-H518 Deep Learning\\Assignments\\A3\\Models\\M2\\assets\n"
     ]
    }
   ],
   "source": [
    "history = kr_words.fit(\n",
    "    train_data,\n",
    "    train_labels,\n",
    "    validation_data=(valid_data, valid_labels),\n",
    "    epochs=epochs\n",
    ")\n",
    "kr_words.save(r'C:\\Users\\samue\\Documents\\Applied Data Science\\INFO-H518 Deep Learning\\Assignments\\A3\\Models\\M2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M2 | Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429/429 [==============================] - 1s 2ms/step - loss: 0.6419 - binary_accuracy: 0.7323\n",
      "Loss: 0.6418583393096924 \n",
      "Accuracy: 0.7323328256607056\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = kr_words.evaluate(test_data, test_labels)\n",
    "print(f\"Loss: {loss} \\nAccuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use K random words from a tweet to predict the tweeter's political party\n",
    "(fill feature vector to length K if len(feature vector) < K)\n",
    "\n",
    "Epoch = 5 \n",
    "\n",
    "K = 20 \n",
    "\n",
    "Embedding_dim = 32 \n",
    "\n",
    "Optimizer = 'adam', lr = 0.002, b1 = 0.8 \n",
    "\n",
    "Loss = BinaryCrossEntropy : 0.6419\n",
    "\n",
    "Accuracy = 0.7323"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model #3: N-Most Frequent Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_chosen_words(words_i:dict, data: pd.DataFrame, n: int):\n",
    "    words_i = {list(train_words_i.keys())[-1:-n:-1][i]: i+1 for i in range(n-1)}\n",
    "    for i, row in data.iterrows():\n",
    "        terms = dict.fromkeys(list(words_i.keys()), np.float32(0.0))\n",
    "        for term in row['Tokens']:\n",
    "            if term in terms.keys(): terms[term] = words_i[term] \n",
    "        x = tf.convert_to_tensor(list(terms.values()), dtype=tf.float32)\n",
    "        y = row['Party']\n",
    "        yield (x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M3 | Build the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Build Training Data...\n",
      "[######################]\n",
      " Build Testing Data...\n",
      "[######################]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1023,), dtype=float32, numpy=array([0., 2., 0., ..., 0., 0., 0.], dtype=float32)>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters\n",
    "epochs = 10\n",
    "embedding_dim = 64\n",
    "n = 1024\n",
    "\n",
    "# Train\n",
    "print(\" Build Training Data...\\n[#\", end='')\n",
    "train_data, train_labels = [], []\n",
    "for x, y in n_chosen_words(train_words_i, train, n):\n",
    "    train_data.append(x)\n",
    "    train_labels.append(y)\n",
    "    if len(train_labels) % int(train.shape[0]*.1) == 0: \n",
    "        print(f\"##\", end='')\n",
    "train_labels = tf.convert_to_tensor(train_labels, dtype=tf.float32)\n",
    "train_data = tf.convert_to_tensor(train_data, dtype=tf.float32)\n",
    "print(\"#]\")\n",
    "\n",
    "# Validation\n",
    "split = round(train_data.shape[0]*.8)\n",
    "valid_data, valid_labels = train_data[split:], train_labels[split:]\n",
    "train_data, train_labels = train_data[:split], train_labels[:split]\n",
    "\n",
    "# Test\n",
    "test_data, test_labels = [], []\n",
    "print(\" Build Testing Data...\\n[#\", end='')\n",
    "for x, y in n_chosen_words(test_words_i, test, n):\n",
    "    test_data.append(x)\n",
    "    test_labels.append(y)\n",
    "    if len(test_labels) % int(test.shape[0]*.1) == 0: \n",
    "        print(f\"##\", end='')\n",
    "test_labels = tf.convert_to_tensor(test_labels, dtype=tf.float32)\n",
    "test_data = tf.cast(test_data, dtype=tf.float32)\n",
    "print(\"#]\")\n",
    "\n",
    "(train_data[0], train_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M3 | Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure\n",
    "choose_n = Sequential([\n",
    "    layers.Embedding(n+1, embedding_dim),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.GlobalAveragePooling1D(),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "choose_n.compile(\n",
    "    loss=losses.BinaryCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.Adam(\n",
    "        learning_rate=0.002,\n",
    "        beta_1=0.8,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-07,\n",
    "        amsgrad=False,\n",
    "        name='Adam'\n",
    "    ),\n",
    "    metrics=tf.metrics.BinaryAccuracy(threshold=0.0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M3 | Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1819/1819 [==============================] - 9s 5ms/step - loss: 0.6920 - binary_accuracy: 0.5180 - val_loss: 0.6889 - val_binary_accuracy: 0.5153\n",
      "Epoch 2/10\n",
      "1819/1819 [==============================] - 9s 5ms/step - loss: 0.6819 - binary_accuracy: 0.5693 - val_loss: 0.6708 - val_binary_accuracy: 0.5609\n",
      "Epoch 3/10\n",
      "1819/1819 [==============================] - 9s 5ms/step - loss: 0.6535 - binary_accuracy: 0.6230 - val_loss: 0.6391 - val_binary_accuracy: 0.6356\n",
      "Epoch 4/10\n",
      "1819/1819 [==============================] - 9s 5ms/step - loss: 0.6240 - binary_accuracy: 0.6465 - val_loss: 0.6119 - val_binary_accuracy: 0.6557\n",
      "Epoch 5/10\n",
      "1819/1819 [==============================] - 9s 5ms/step - loss: 0.6049 - binary_accuracy: 0.6602 - val_loss: 0.6069 - val_binary_accuracy: 0.6478\n",
      "Epoch 6/10\n",
      "1819/1819 [==============================] - 9s 5ms/step - loss: 0.5948 - binary_accuracy: 0.6657 - val_loss: 0.5901 - val_binary_accuracy: 0.6674\n",
      "Epoch 7/10\n",
      "1819/1819 [==============================] - 9s 5ms/step - loss: 0.5877 - binary_accuracy: 0.6699 - val_loss: 0.5927 - val_binary_accuracy: 0.6592\n",
      "Epoch 8/10\n",
      "1819/1819 [==============================] - 8s 5ms/step - loss: 0.5825 - binary_accuracy: 0.6744 - val_loss: 0.5826 - val_binary_accuracy: 0.6720\n",
      "Epoch 9/10\n",
      "1819/1819 [==============================] - 9s 5ms/step - loss: 0.5800 - binary_accuracy: 0.6753 - val_loss: 0.5847 - val_binary_accuracy: 0.6663\n",
      "Epoch 10/10\n",
      "1819/1819 [==============================] - 9s 5ms/step - loss: 0.5769 - binary_accuracy: 0.6763 - val_loss: 0.5783 - val_binary_accuracy: 0.6751\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\samue\\Documents\\Applied Data Science\\INFO-H518 Deep Learning\\Assignments\\A3\\Models\\M3\\assets\n"
     ]
    }
   ],
   "source": [
    "history = choose_n.fit(\n",
    "    train_data,\n",
    "    train_labels,\n",
    "    validation_data=(valid_data, valid_labels),\n",
    "    epochs=epochs\n",
    ")\n",
    "choose_n.save(r'C:\\Users\\samue\\Documents\\Applied Data Science\\INFO-H518 Deep Learning\\Assignments\\A3\\Models\\M3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M3 | Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429/429 [==============================] - 1s 2ms/step - loss: 0.5778 - binary_accuracy: 0.6708\n",
      "Loss: 0.5777549147605896 \n",
      "Accuracy: 0.6708436608314514\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = choose_n.evaluate(test_data, test_labels)\n",
    "print(f\"Loss: {loss} \\nAccuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use N words from the vocabulary with values set to zero UNLESS the value appears in the tweet of interest. Use this feature vector to predict the political party of the user.\n",
    "\n",
    "Epoch = 10\n",
    "\n",
    "N = 1024\n",
    "\n",
    "Embedding_dim = 64\n",
    "\n",
    "Optimizer = Adam\n",
    "\n",
    "Loss = 0.5778\n",
    "\n",
    "Accuracy = 0.6708"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model #4: K-random-words with Handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_random_k_words(words_i:dict, data: pd.DataFrame, k: int):\n",
    "    for i, row in data.iterrows():\n",
    "        terms = []\n",
    "        for _ in range(k):\n",
    "            if len(row['Tokens']) > 0:\n",
    "                token = random.choice(row['Tokens'])\n",
    "                val = words_i[token] \\\n",
    "                    if token in words_i.keys() \\\n",
    "                    else np.float32(0)\n",
    "                terms.append(val)\n",
    "            else:\n",
    "                terms = [np.float32(0) for _ in range(k)]\n",
    "                break\n",
    "        x = np.array(terms+[np.float32(row['Handle'])], dtype=np.float32)\n",
    "        y = row['Party']\n",
    "        yield (x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M4 | Build the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Build Training Data...\n",
      "[######################]\n",
      " Build Testing Data...\n",
      "[######################]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(257,), dtype=float32, numpy=\n",
       " array([63854., 63858., 63871., 63874., 62282., 63120., 63120., 63647.,\n",
       "        63647., 63120., 63854., 63746., 61277., 63817., 63725., 63784.,\n",
       "        63120., 63784., 63120., 63703., 63854., 63647., 61277., 63863.,\n",
       "        63120., 63822., 63422., 63871., 62282., 63854., 63725., 63647.,\n",
       "        61277., 63422., 63647., 63422., 63871., 63854., 63442., 63858.,\n",
       "        63869., 63746., 62282., 63442., 63874., 63725., 63858., 61277.,\n",
       "        63871., 62282., 63863., 63784., 63703., 63822., 63746., 63120.,\n",
       "        63863., 63854., 63784., 63422., 63703., 63817., 63746., 63874.,\n",
       "        63746., 63874., 63871., 63422., 62282., 63817., 62282., 63858.,\n",
       "        63120., 61277., 63822., 63120., 63858., 63869., 63703., 63647.,\n",
       "        63120., 61277., 63863., 63869., 63725., 63869., 63784., 63817.,\n",
       "        63854., 63817., 61277., 63858., 61277., 63817., 63725., 63854.,\n",
       "        61277., 63422., 63817., 63817., 63784., 63647., 63422., 63422.,\n",
       "        63858., 61277., 63874., 63784., 63746., 61277., 63869., 63863.,\n",
       "        63725., 63784., 63647., 63746., 63817., 63422., 63858., 63120.,\n",
       "        61277., 63858., 63871., 63863., 63871., 63120., 63858., 63422.,\n",
       "        63822., 63817., 63822., 63854., 63863., 63858., 63863., 63869.,\n",
       "        63854., 63703., 63858., 63858., 63784., 63647., 62282., 63822.,\n",
       "        61277., 63858., 63703., 63871., 61277., 63858., 63869., 63874.,\n",
       "        62282., 63784., 63817., 63746., 61277., 63854., 63854., 63647.,\n",
       "        63822., 63422., 63869., 61277., 63871., 63746., 63863., 63854.,\n",
       "        63822., 63854., 63863., 63422., 63120., 63817., 63858., 63746.,\n",
       "        63871., 63874., 63869., 63863., 63784., 63854., 63817., 62282.,\n",
       "        62282., 62282., 63746., 63863., 63871., 63746., 63725., 63858.,\n",
       "        62282., 63120., 63854., 62282., 63703., 63647., 63422., 63858.,\n",
       "        63874., 63874., 63784., 63871., 63871., 63822., 63822., 63869.,\n",
       "        63871., 63822., 63422., 63874., 63442., 63746., 62282., 61277.,\n",
       "        63784., 63725., 63871., 63822., 63422., 63784., 63871., 63422.,\n",
       "        61277., 63854., 63784., 63703., 63863., 61277., 63854., 63703.,\n",
       "        63854., 63703., 63746., 63120., 63874., 63854., 61277., 63858.,\n",
       "        63854., 63784., 63422., 63784., 62282., 63442., 63120., 63817.,\n",
       "        63854., 63422., 63120., 63442., 63442., 63869., 63703., 61277.,\n",
       "          333.], dtype=float32)>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters\n",
    "epochs = 10\n",
    "embedding_dim = 256\n",
    "k = 256\n",
    "\n",
    "# Train\n",
    "print(\" Build Training Data...\\n[#\", end='')\n",
    "train_data, train_labels = [], []\n",
    "for x, y in handle_random_k_words(train_words_i, train, k):\n",
    "    train_data.append(x)\n",
    "    train_labels.append(y)\n",
    "    if len(train_labels) % int(train.shape[0]*.1) == 0: \n",
    "        print(f\"##\", end='')\n",
    "train_labels = tf.convert_to_tensor(train_labels, dtype=tf.float32)\n",
    "train_data = tf.cast(train_data, dtype=tf.float32)\n",
    "print(\"#]\")\n",
    "\n",
    "# Validation\n",
    "split = round(train_data.shape[0]*.8)\n",
    "valid_data, valid_labels = train_data[split:], train_labels[split:]\n",
    "train_data, train_labels = train_data[:split], train_labels[:split]\n",
    "\n",
    "# Test\n",
    "print(\" Build Testing Data...\\n[#\", end='')\n",
    "test_data, test_labels = [], []\n",
    "for x, y in handle_random_k_words(test_words_i, test, k):\n",
    "    test_data.append(x)\n",
    "    test_labels.append(y)\n",
    "    if len(test_labels) % int(test.shape[0]*.1) == 0: \n",
    "        print(f\"##\", end='')\n",
    "test_labels = tf.convert_to_tensor(test_labels, dtype=tf.float32)\n",
    "test_data = tf.cast(test_data, dtype=tf.float32)\n",
    "print(\"#]\")\n",
    "\n",
    "(train_data[0], train_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M4 | Create the structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure\n",
    "handle_kr_words = Sequential([\n",
    "    layers.Embedding(len(vocab)+1, embedding_dim),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.GlobalAveragePooling1D(),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "handle_kr_words.compile(\n",
    "    loss=losses.BinaryCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.Adam(\n",
    "        learning_rate=0.001,\n",
    "        beta_1=0.85,\n",
    "        beta_2=0.990,\n",
    "        epsilon=1e-05,\n",
    "        amsgrad=False,\n",
    "        name='Adam'\n",
    "    ),\n",
    "    metrics=tf.metrics.BinaryAccuracy(threshold=0.0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M4 | Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "910/910 [==============================] - 10s 10ms/step - loss: 0.5600 - binary_accuracy: 0.7108 - val_loss: 0.4629 - val_binary_accuracy: 0.7718\n",
      "Epoch 2/10\n",
      "910/910 [==============================] - 9s 10ms/step - loss: 0.3712 - binary_accuracy: 0.8364 - val_loss: 0.4122 - val_binary_accuracy: 0.8093\n",
      "Epoch 3/10\n",
      "910/910 [==============================] - 9s 10ms/step - loss: 0.2782 - binary_accuracy: 0.8867 - val_loss: 0.4045 - val_binary_accuracy: 0.8269\n",
      "Epoch 4/10\n",
      "910/910 [==============================] - 9s 10ms/step - loss: 0.2208 - binary_accuracy: 0.9163 - val_loss: 0.4122 - val_binary_accuracy: 0.8341\n",
      "Epoch 5/10\n",
      "910/910 [==============================] - 9s 10ms/step - loss: 0.1813 - binary_accuracy: 0.9349 - val_loss: 0.4224 - val_binary_accuracy: 0.8400\n",
      "Epoch 6/10\n",
      "910/910 [==============================] - 9s 10ms/step - loss: 0.1520 - binary_accuracy: 0.9473 - val_loss: 0.4376 - val_binary_accuracy: 0.8436\n",
      "Epoch 7/10\n",
      "910/910 [==============================] - 9s 10ms/step - loss: 0.1298 - binary_accuracy: 0.9563 - val_loss: 0.4443 - val_binary_accuracy: 0.8530\n",
      "Epoch 8/10\n",
      "910/910 [==============================] - 9s 10ms/step - loss: 0.1125 - binary_accuracy: 0.9639 - val_loss: 0.4637 - val_binary_accuracy: 0.8514\n",
      "Epoch 9/10\n",
      "910/910 [==============================] - 9s 10ms/step - loss: 0.0988 - binary_accuracy: 0.9693 - val_loss: 0.4812 - val_binary_accuracy: 0.8550\n",
      "Epoch 10/10\n",
      "910/910 [==============================] - 9s 10ms/step - loss: 0.0876 - binary_accuracy: 0.9738 - val_loss: 0.4948 - val_binary_accuracy: 0.8553\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\samue\\Documents\\Applied Data Science\\INFO-H518 Deep Learning\\Assignments\\A3\\Models\\M4\\assets\n"
     ]
    }
   ],
   "source": [
    "history = handle_kr_words.fit(\n",
    "    train_data,\n",
    "    train_labels,\n",
    "    validation_data=(valid_data, valid_labels),\n",
    "    epochs=epochs,\n",
    "    batch_size=64\n",
    ")\n",
    "handle_kr_words.save(r'C:\\Users\\samue\\Documents\\Applied Data Science\\INFO-H518 Deep Learning\\Assignments\\A3\\Models\\M4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M4 | Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429/429 [==============================] - 1s 2ms/step - loss: 0.7322 - binary_accuracy: 0.7758\n",
      "Loss: 0.7322496175765991 \n",
      "Accuracy: 0.7758268713951111\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = handle_kr_words.evaluate(test_data, test_labels)\n",
    "print(f\"Loss: {loss} \\nAccuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #5: First K-words with Handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_fk_words(words_i:dict, data: pd.DataFrame, k: int):\n",
    "    for i, row in data.iterrows():\n",
    "        terms, token_count = [], len(row['Tokens'])\n",
    "        if token_count > 0:\n",
    "            j = 0\n",
    "            while len(terms) < k:\n",
    "                if row['Tokens'][j] in words_i.keys():\n",
    "                    terms.append(words_i[row['Tokens'][j]])\n",
    "                else:\n",
    "                    terms.append(np.float32(0))\n",
    "                j = j+1 if j+1 < token_count else 0\n",
    "        else:\n",
    "            continue\n",
    "        x = np.array(terms+[np.float32(row['Handle'])], dtype=np.float32)\n",
    "        y = row['Party']\n",
    "        yield (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([408, 415,  36,  52, 230, 337, 331, 418, 367, 228, 121,  74, 324,\n",
       "       255, 205,  98, 283, 220, 134, 222,  10, 240, 333, 139, 160,  93,\n",
       "       417, 243, 391,  50, 247, 236, 290,   3, 298, 182, 378, 244, 128,\n",
       "       270, 421, 289,  63,  21, 322, 354,  40, 214, 335, 392, 149, 286,\n",
       "       288, 225, 280, 365, 241, 217, 159, 143, 138, 406, 261,  60,  12,\n",
       "       302, 178, 104,  35, 154, 284, 266, 265, 133, 193,  66, 151, 248,\n",
       "       136, 390, 282, 400,  91, 371,  19, 328,  47, 140, 314, 281, 357,\n",
       "       109, 350, 430,  49, 315,  94, 277, 377, 207, 211,  20, 124, 410,\n",
       "       148, 300, 113, 344, 258,   4,  55,  70, 239, 276,  13, 338, 112,\n",
       "        26, 419, 368, 242,  81, 231, 273, 330, 353, 246, 147, 360, 183,\n",
       "       101, 186,  15,  34,  72, 278, 166, 385, 105, 122, 257,   2,  71,\n",
       "        68,  65, 201, 126,  29, 363,  24, 414, 125, 170, 127,  43, 346,\n",
       "        84, 299, 108,  11, 249, 413, 123, 309, 224, 342,  25, 194, 210,\n",
       "       422, 389,  22, 187,  87, 411, 326, 252, 374, 232, 293, 317,  42,\n",
       "       376, 264, 412, 384, 198, 423, 320, 361, 311,  41,  28, 348, 308,\n",
       "       424, 366, 399,  79, 191,  88, 396, 155,  85, 307, 295, 429, 409,\n",
       "       287, 199,   1, 395,  53, 181, 370,   9, 110, 272, 313,  58, 215,\n",
       "       275, 213, 174,   5,  86,  96, 347, 297, 254, 379, 137, 301, 141,\n",
       "       432, 135,  99, 132, 349, 343, 219, 291, 116,   0, 253, 387, 144,\n",
       "       233, 427, 120, 179, 398, 102, 192, 312,  31,   8, 352, 310,  32,\n",
       "        59, 316, 372, 221, 294, 153, 229, 169, 303,  39, 177, 334, 204,\n",
       "       118,  77, 358, 237,  82, 218, 259, 175,   6,  95, 323, 176, 404,\n",
       "       325,  17, 401,  76, 388, 158, 380, 212, 238, 100, 431, 336, 115,\n",
       "       383, 208, 397, 381,  51, 327, 262, 223, 364,  57,  45, 268, 359,\n",
       "        64, 393, 420, 171, 340, 180, 146, 168, 369, 145, 162, 269,  92,\n",
       "       305, 405, 130, 185,  30, 428, 202, 345,   7, 356, 200, 161, 416,\n",
       "       319, 117, 119, 329, 256, 318, 156, 235, 197, 203, 426, 341,  75,\n",
       "        97, 234,  90,  27, 251,  46, 106, 196,  78, 274,  61, 165, 263,\n",
       "       114, 164,  33, 362,  69, 332,  80, 351, 386,  16, 107, 292, 209,\n",
       "        38, 226, 304,  73,  56, 306,  44,  67, 279, 195, 129, 163,  48,\n",
       "       321, 188, 103, 271,  54, 382, 111, 245, 152, 172, 394,  83, 131,\n",
       "       260, 373,  89, 167, 296, 189, 216, 206, 407, 267, 425, 142, 150,\n",
       "       173, 157,  23,  18, 250, 375,  37, 227, 190, 184, 285, 355, 403,\n",
       "        62, 339,  14, 402], dtype=int16)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_hands = train.rename(columns={'Handle':'Terms'})['Terms'].unique()\n",
    "train_hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Build Training Data...\n",
      "[####################]\n",
      " Build Testing Data...\n",
      "[####################]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(11,), dtype=float32, numpy=\n",
       " array([63422., 63869., 63120., 63874., 63647., 61277., 63746., 63703.,\n",
       "        63784., 63871.,   333.], dtype=float32)>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters\n",
    "epochs = 5\n",
    "embedding_dim = 32\n",
    "k = 10\n",
    "\n",
    "# # Remove 10% of the vocabulary (specifcially infrequent terms)\n",
    "train_hands = train.Handle.unique().rename(columns={'Handle':'Terms'})\n",
    "# vocab = train_vocab.append(test_vocab).append(train['Handle']).append(test['Handle'].reset_index()\n",
    "# vocab_cut = int(train_vocab.shape[0] * (10 / 100))\n",
    "# vocab = vocab.iloc[vocab_cut:].drop_duplicates('Terms').reset_index().drop(columns=['index', 'level_0'])\n",
    "# vocab = {v:k for k, v in vocab.to_dict()['Terms'].items()}\n",
    "\n",
    "# # Provide each term a unique id\n",
    "# train_words_i = {v:vocab[v] for v in list(train_vocab.to_dict()['Terms'].values()) if v in vocab.keys()}\n",
    "# test_words_i = {v:vocab[v] for v in list(test_vocab.to_dict()['Terms'].values()) if v in vocab.keys()}\n",
    "\n",
    "# Train\n",
    "print(\" Build Training Data...\\n[#\", end='')\n",
    "train_data, train_labels = [], []\n",
    "for x, y in handle_fk_words(train_words_i, train, k):\n",
    "    train_data.append(x)\n",
    "    train_labels.append(y)\n",
    "    if len(train_labels) % int(train.shape[0]*.1) == 0: \n",
    "        print(f\"##\", end='')\n",
    "train_labels = tf.convert_to_tensor(train_labels, dtype=tf.float32)\n",
    "train_data = tf.cast(train_data, dtype=tf.float32)\n",
    "print(\"#]\")\n",
    "\n",
    "# Validation\n",
    "split = round(train_data.shape[0]*.8)\n",
    "valid_data, valid_labels = train_data[split:], train_labels[split:]\n",
    "train_data, train_labels = train_data[:split], train_labels[:split]\n",
    "\n",
    "# Test\n",
    "print(\" Build Testing Data...\\n[#\", end='')\n",
    "test_data, test_labels = [], []\n",
    "for x, y in handle_fk_words(test_words_i, test, k):\n",
    "    test_data.append(x)\n",
    "    test_labels.append(y)\n",
    "    if len(test_labels) % int(test.shape[0]*.1) == 0: \n",
    "        print(f\"##\", end='')\n",
    "test_labels = tf.convert_to_tensor(test_labels, dtype=tf.float32)\n",
    "test_data = tf.cast(test_data, dtype=tf.float32)\n",
    "print(\"#]\")\n",
    "\n",
    "(train_data[0], train_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure\n",
    "handle_k_words = Sequential([\n",
    "    layers.Embedding(len(vocab)+1, embedding_dim),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.GlobalAveragePooling1D(),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "handle_k_words.compile(\n",
    "    loss=losses.BinaryCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.Adam(\n",
    "        learning_rate=0.001,\n",
    "        beta_1=0.95,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-07,\n",
    "        amsgrad=False,\n",
    "        name='Adam'\n",
    "    ),\n",
    "    metrics=tf.metrics.BinaryAccuracy(threshold=0.0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1812/1812 [==============================] - 10s 5ms/step - loss: 0.3375 - binary_accuracy: 0.9315 - val_loss: 0.0764 - val_binary_accuracy: 0.9982\n",
      "Epoch 2/5\n",
      "1812/1812 [==============================] - 9s 5ms/step - loss: 0.0318 - binary_accuracy: 0.9991 - val_loss: 0.0147 - val_binary_accuracy: 0.9992\n",
      "Epoch 3/5\n",
      "1812/1812 [==============================] - 8s 4ms/step - loss: 0.0076 - binary_accuracy: 0.9997 - val_loss: 0.0056 - val_binary_accuracy: 0.9992\n",
      "Epoch 4/5\n",
      "1812/1812 [==============================] - 8s 5ms/step - loss: 0.0029 - binary_accuracy: 0.9999 - val_loss: 0.0030 - val_binary_accuracy: 0.9993\n",
      "Epoch 5/5\n",
      "1812/1812 [==============================] - 9s 5ms/step - loss: 0.0013 - binary_accuracy: 0.9999 - val_loss: 0.0023 - val_binary_accuracy: 0.9992\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\samue\\Documents\\Applied Data Science\\INFO-H518 Deep Learning\\Assignments\\A3\\Models\\M4\\assets\n"
     ]
    }
   ],
   "source": [
    "history = handle_k_words.fit(\n",
    "    train_data,\n",
    "    train_labels,\n",
    "    validation_data=(valid_data, valid_labels),\n",
    "    epochs=epochs\n",
    ")\n",
    "handle_k_words.save(r'C:\\Users\\samue\\Documents\\Applied Data Science\\INFO-H518 Deep Learning\\Assignments\\A3\\Models\\M4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "428/428 [==============================] - 1s 2ms/step - loss: 2.7048 - binary_accuracy: 0.5714\n",
      "Loss: 2.704810857772827 \n",
      "Accuracy: 0.571407675743103\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = handle_k_words.evaluate(test_data, test_labels)\n",
    "print(f\"Loss: {loss} \\nAccuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL 1:\n",
    "\n",
    "Use FIRST K words from a tweet to predict the tweeter's political party\n",
    "(fill feature vector to length K if len(feature vector) < K)\n",
    "\n",
    "Epoch = 5 \n",
    "\n",
    "K = 20 \n",
    "\n",
    "Embedding_dim = 32 \n",
    "\n",
    "Optimizer = 'adam', lr = .002, b1 = .8 \n",
    "\n",
    "Loss = .6934\n",
    "\n",
    "Accuracy = .5060"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL 2:\n",
    "\n",
    "Use K random words from a tweet to predict the tweeter's political party\n",
    "(fill feature vector to length K if len(feature vector) < K)\n",
    "\n",
    "Epoch = 5 \n",
    "\n",
    "K = 20 \n",
    "\n",
    "Embedding_dim = 32 \n",
    "\n",
    "Optimizer = 'adam', lr = 0.002, b1 = 0.8 \n",
    "\n",
    "Loss = BinaryCrossEntropy : 0.6932\n",
    "\n",
    "Accuracy = 0.5060"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL 3:\n",
    "Use N words from the vocabulary with values set to zero UNLESS the value appears in the tweet of interest. Use this feature vector to predict the political party of the user.\n",
    "\n",
    "Epoch = 10\n",
    "\n",
    "N = 1000\n",
    "\n",
    "Embedding_dim = 32\n",
    "\n",
    "Optimizer = 'adam', lr = 0.002, b1 = 0.8 \n",
    "\n",
    "Loss = 0.6931\n",
    "\n",
    "Accuracy = 0.5060"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL 4: TAKE 1\n",
    "Use K random words PLUS the user's Handle from a tweet to predict the user's political party.\n",
    "Additionally, we will randomly sample K words and pick a new word if the word at hand is not in the vocabulary, as opposed to selecting 0 as the token. \n",
    "\n",
    "Epoch = 10\n",
    "\n",
    "K = 64 \n",
    "\n",
    "Embedding_dim = 128 \n",
    "\n",
    "Optimizer = 'adam', lr = 0.001, b1 = 0.85\n",
    "\n",
    "Loss: 1.470948338508606 \n",
    "\n",
    "Accuracy: 0.5401427745819092\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL 4: TAKE 2\n",
    "Clearly, we are over fitting since our training accuracy and loss are drastically different from our testing results.\n",
    "Let's try increasing the dropout.\n",
    "\n",
    "Double the dropout percentage in D1 from .1 to .2 \n",
    "\n",
    "Double the dropout percentage in D2 from .2 to .4\n",
    "\n",
    "RESULTS:\n",
    "\n",
    "Loss: 1.4692225456237793 \n",
    "\n",
    "Accuracy: 0.5401427745819092"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL 4: TAKE 3\n",
    "Since increasing Dropout didn't work, let's try decreasing it.\n",
    "\n",
    "Removed D2.\n",
    "\n",
    "RESULTS:\n",
    "\n",
    "Train: Loss: 0.4890 | Accuracy: 0.6533\n",
    "\n",
    "Test: Loss: 1.2920 | Accuracy: 0.5496"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL 4: TAKE 4\n",
    "Let's try changing over to an optimizer that is a bit simple when compared to Adam. With SGD we will use the same learning rate and initially 0.0 momentum. Just to see where we are at.\n",
    "\n",
    "RESULT:\n",
    "\n",
    "Though SGD did solve our overfitting problem, it also cost us in test performance too as Accuracy was only 0.0506\n",
    "\n",
    "Unfortunately, I couldn't get RMSProp to work correctly with CUDA on the GPU and CPU, so instead I want to see if I keep increasing the sample size k, does test accuracy also increase with training while using Adam.\n",
    "\n",
    "RESULT:\n",
    "\n",
    "By increasing k to 256, \n",
    "\n",
    "decreasing lr to 0.001,\n",
    "\n",
    "decreasing b2 to 0.990,\n",
    "\n",
    "and increasing epsilon to 1e-5 (up from 1e-7)\n",
    "\n",
    "We were able to get\n",
    "\n",
    "Loss: 0.8411399126052856 \n",
    "\n",
    "Accuracy: 0.5572636127471924"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
