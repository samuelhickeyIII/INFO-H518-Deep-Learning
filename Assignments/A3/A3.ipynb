{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data description\n",
    "This dataset has three columns - label (party name), twitter handle, tweet text\n",
    "\n",
    "\n",
    "#### Problem Description:\n",
    "\n",
    "Design a feed forward deep neural network to predict the political party using the pytorch or tensorflow. \n",
    "Build two models\n",
    "\n",
    "1. Without using the handle\n",
    "\n",
    "2. Using the handle\n",
    "\n",
    "\n",
    "#### Deliverables\n",
    "\n",
    "- Report the performance on the test set.\n",
    "\n",
    "- Try multiple models and with different hyperparameters. Present the results of each model on the test set. No need to create a dev set.\n",
    "\n",
    "- Experiment with:\n",
    "    -L2 and dropout regularization techniques\n",
    "    -SGD, RMSProp and Adamp optimization techniques\n",
    "\n",
    "\n",
    "\n",
    "- Creating a fixed-sized vocabulary: Give a unique id to each word in your selected vocabulary and use it as the input to the network\n",
    "\n",
    "    - Option 1: Feedforward networks can only handle fixed-sized inputs. You can choose to have a fixed-sized K words from the tweet text (e.g. the first K word, randomly selected K word etc.). K can be a hyperparameter. \n",
    "\n",
    "    - Option 2: you can choose top N (e.g. N=1000) frequent words from the dataset and use an N-sized input layer. If a word is present in a tweet, pass the id, 0 otherwise\n",
    "    \n",
    "    -  Clearly state your design choices and assumptions. Think about the pros and cons of each option.\n",
    "\n",
    " \n",
    "\n",
    "<b> Tabulate your results, either at the end of the code file or in the text box on the submission page. The final result should have:</b>\n",
    "\n",
    "1. Experiment description\n",
    "\n",
    "2. Hyperparameter used and their values\n",
    "\n",
    "3. Performance on the test set\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, losses\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import text\n",
    "from keras.utils import np_utils\n",
    "from random import randrange\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "PATH = r\"C:\\Users\\samue\\Documents\\Applied Data Science\\INFO-H518 Deep Learning\\Assignments\\A3\\Input\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Retrieval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the data\n",
    "train = pd.read_pickle(PATH + r'\\train_tokenized.pickle').dropna().sample(frac=1)\n",
    "train_vocab = pd.read_csv(PATH + r'\\train_vocab_frequency.csv', index_col=0) \\\n",
    "    .dropna().drop(columns='Count')\n",
    "test = pd.read_pickle(PATH + r'\\test_tokenized.pickle').dropna().sample(frac=1)\n",
    "test_vocab = pd.read_csv(PATH + r'\\test_vocab_frequency.csv', index_col=0) \\\n",
    "    .dropna().drop(columns='Count')\n",
    "\n",
    "train['Party'] = pd.Categorical(train.Party)\n",
    "train['Party'] = train.Party.cat.codes\n",
    "test['Party'] = pd.Categorical(test.Party)\n",
    "test['Party'] = test.Party.cat.codes\n",
    "\n",
    "vocab_size = train_vocab['Terms'].append(test_vocab['Terms']).unique().shape[0]\n",
    "\n",
    "# Validation split\n",
    "idx = int(train.shape[0]*.8)\n",
    "valid = train.iloc[idx:]\n",
    "train = train.iloc[:idx]\n",
    "\n",
    "# Format into Tensors\n",
    "train_dataset = tf.data.Dataset \\\n",
    "    .from_tensor_slices(\n",
    "        (train['Tweet'].to_numpy(), train['Party'].to_numpy())\n",
    "    )\n",
    "\n",
    "valid_dataset = tf.data.Dataset \\\n",
    "    .from_tensor_slices(\n",
    "        (valid['Tweet'].to_numpy(), valid['Party'].to_numpy())\n",
    "    )\n",
    "\n",
    "test_dataset = tf.data.Dataset \\\n",
    "    .from_tensor_slices(\n",
    "        (test['Tweet'].to_numpy(), test['Party'].to_numpy())\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Example\n",
      "Tweet: b'Fun to be at the Congressional Baseball game with my family but my friend Steve Scalise was deeply missed. https://t.co/6MFTima9YM' \n",
      "Label: 1\n",
      "\n",
      "Validation Example\n",
      "Tweet: b'On this day, holy for so many, let us be joined by our common values https://t.co/iO9FmQuYgW' \n",
      "Label: 0\n",
      "\n",
      "Test Example\n",
      "Tweet: b\"I'm chairing the Tactical Air &amp; Land Forces #FY19NDAA markup. You can watch live here: https://t.co/omDPZCTzyc\" \n",
      "Label: 1\n"
     ]
    }
   ],
   "source": [
    "# Set up the input pipeline\n",
    "BUFFER_SIZE = 5000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = train_dataset \\\n",
    "    .shuffle(BUFFER_SIZE) \\\n",
    "    .batch(BATCH_SIZE)\n",
    "\n",
    "valid_dataset = valid_dataset \\\n",
    "    .shuffle(BUFFER_SIZE) \\\n",
    "    .batch(BATCH_SIZE)\n",
    "\n",
    "test_dataset = test_dataset \\\n",
    "    .shuffle(BUFFER_SIZE) \\\n",
    "    .batch(BATCH_SIZE)\n",
    "\n",
    "print('Train Example')\n",
    "for tweet, label in train_dataset.take(1):\n",
    "    print(f\"Tweet: {tweet[0]} \\nLabel: {label[0]}\")\n",
    "\n",
    "print('\\nValidation Example')\n",
    "for tweet, label in valid_dataset.take(1):\n",
    "    print(f\"Tweet: {tweet[0]} \\nLabel: {label[0]}\")\n",
    "\n",
    "print('\\nTest Example')\n",
    "for tweet, label in test_dataset.take(1):\n",
    "    print(f\"Tweet: {tweet[0]} \\nLabel: {label[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: b'On  cspan LIVE soon  I ll be on the floor to talk about my H Con Res      Encouraging reunions of divided Korean American families' \n",
      "Label: 0\n"
     ]
    }
   ],
   "source": [
    "def cleaned_data(input_data):\n",
    "    stripped_url_ending = tf.strings.regex_replace(input_data, \"https(.*)\", '')\n",
    "    words_and_tags = tf.strings.regex_replace(stripped_url_ending, \"[^#A-Za-z]\", ' ')\n",
    "    return tf.strings.regex_replace(\n",
    "        words_and_tags,\n",
    "        '[%s]' % re.escape(string.punctuation[:2]+string.punctuation[3:]),\n",
    "        ''\n",
    "    )\n",
    "\n",
    "for tweet, label in test_dataset.take(1):\n",
    "    print(f\"Tweet: {cleaned_data(tweet[0])} \\nLabel: {label[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet:  b'RT @IRSnews: #IRSTaxTip: If you didn\\xe2\\x80\\x99t get your #IRS #tax refund yet, check its status with \\xe2\\x80\\x9cWhere\\xe2\\x80\\x99s My Refund?\\xe2\\x80\\x9d https://t.co/Z07vIsuvQf'\n",
      "Label:  1\n",
      "Encoded Tweet:  (<tf.Tensor: shape=(1, 25), dtype=int64, numpy=\n",
      "array([[    9,  5943,     1,   171,    18,  1196,    32,   160,    46,\n",
      "         2884, 24480, 13160,   626,  1158,   167,  1282,    14,  3314,\n",
      "           11,    73,     1,     0,     0,     0,     0]], dtype=int64)>, <tf.Tensor: shape=(), dtype=int8, numpy=1>)\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "k = 25\n",
    "\n",
    "# Vectorization Layer\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize=cleaned_data,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=k\n",
    ")\n",
    "\n",
    "train_features = train_dataset.map(lambda x, y: x)\n",
    "valid_features = valid_dataset.map(lambda x, y: x)\n",
    "test_features = test_dataset.map(lambda x, y: x) \n",
    "\n",
    "vectorize_layer.adapt(train_features)\n",
    "vectorize_layer.adapt(valid_features)\n",
    "vectorize_layer.adapt(test_features)\n",
    "\n",
    "def vectorize_me(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text), label\n",
    "\n",
    "text_batch, label_batch = next(iter(train_dataset))\n",
    "first_review, first_label = text_batch[0], label_batch[0]\n",
    "print(\"Tweet: \", first_review.numpy())\n",
    "print(\"Label: \", first_label.numpy())\n",
    "print(\"Encoded Tweet: \", vectorize_me(first_review, first_label))\n",
    "\n",
    "training = train_dataset.map(vectorize_me).cache().prefetch(tf.data.AUTOTUNE)\n",
    "validation = valid_dataset.map(vectorize_me).prefetch(tf.data.AUTOTUNE)\n",
    "testing = test_dataset.map(vectorize_me).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select K-words from the tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model #1 | Dropout regularization with standard Adam optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M1 | Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "embedding_dim = 32\n",
    "\n",
    "# Structure\n",
    "k_words = Sequential([\n",
    "    layers.Embedding(vocab_size, embedding_dim),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.GlobalMaxPooling1D(),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "k_words.compile(\n",
    "    loss=losses.BinaryCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.Adam(\n",
    "        learning_rate=0.001,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-07,\n",
    "        amsgrad=False,\n",
    "        name='Adam'\n",
    "    ),\n",
    "    metrics=tf.metrics.BinaryAccuracy(threshold=0.0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M1 | Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1819/1819 [==============================] - 10s 5ms/step - loss: 0.6078 - binary_accuracy: 0.6715 - val_loss: 0.5121 - val_binary_accuracy: 0.7485\n",
      "Epoch 2/5\n",
      "1819/1819 [==============================] - 8s 5ms/step - loss: 0.4581 - binary_accuracy: 0.7817 - val_loss: 0.4482 - val_binary_accuracy: 0.7758\n",
      "Epoch 3/5\n",
      "1819/1819 [==============================] - 9s 5ms/step - loss: 0.3898 - binary_accuracy: 0.8203 - val_loss: 0.4322 - val_binary_accuracy: 0.7830\n",
      "Epoch 4/5\n",
      "1819/1819 [==============================] - 9s 5ms/step - loss: 0.3545 - binary_accuracy: 0.8403 - val_loss: 0.4334 - val_binary_accuracy: 0.7873\n",
      "Epoch 5/5\n",
      "1819/1819 [==============================] - 9s 5ms/step - loss: 0.3301 - binary_accuracy: 0.8512 - val_loss: 0.4380 - val_binary_accuracy: 0.7875\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\samue\\Documents\\Applied Data Science\\INFO-H518 Deep Learning\\Assignments\\A3\\Models\\M1\\assets\n"
     ]
    }
   ],
   "source": [
    "history = k_words.fit(\n",
    "    training,\n",
    "    validation_data=validation,\n",
    "    epochs=epochs\n",
    ")\n",
    "k_words.save(r'C:\\Users\\samue\\Documents\\Applied Data Science\\INFO-H518 Deep Learning\\Assignments\\A3\\Models\\M1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M1 | Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429/429 [==============================] - 1s 2ms/step - loss: 0.4155 - binary_accuracy: 0.8021\n",
      "Loss: 0.4155210554599762 \n",
      "Accuracy: 0.8021273612976074\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = k_words.evaluate(testing)\n",
    "print(f\"Loss: {loss} \\nAccuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epochs = 5 \n",
    "\n",
    "K = 25\n",
    "\n",
    "Layers = [embedding | dropout(0.1) | globalAvgPooling1D | dropout(0.3) | dense(1)]\n",
    "\n",
    "Embedding_dim = 32 \n",
    "\n",
    "Optimizer = 'adam' (standard settings)\n",
    "\n",
    "Loss = .4155\n",
    "\n",
    "Accuracy = 0.8021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model #2 | L2 Regularization instead of Dropout "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M2 | Create the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "epochs = 5\n",
    "embedding_dim = 32\n",
    "\n",
    "# Structure\n",
    "kr_words = Sequential([\n",
    "    layers.Embedding(vocab_size, embedding_dim),\n",
    "    layers.GlobalAveragePooling1D(),\n",
    "    layers.Dense(16, activation='swish', kernel_regularizer='l2'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "kr_words.compile(\n",
    "    loss=losses.BinaryCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.Adam(\n",
    "        learning_rate=0.001,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-07,\n",
    "        amsgrad=False,\n",
    "        name='Adam'\n",
    "    ),\n",
    "    metrics=tf.metrics.BinaryAccuracy(threshold=0.0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M2 | Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1819/1819 [==============================] - 10s 5ms/step - loss: 0.6304 - binary_accuracy: 0.6689 - val_loss: 0.5326 - val_binary_accuracy: 0.7522\n",
      "Epoch 2/5\n",
      "1819/1819 [==============================] - 9s 5ms/step - loss: 0.4713 - binary_accuracy: 0.7950 - val_loss: 0.4793 - val_binary_accuracy: 0.7771\n",
      "Epoch 3/5\n",
      "1819/1819 [==============================] - 9s 5ms/step - loss: 0.4104 - binary_accuracy: 0.8275 - val_loss: 0.4617 - val_binary_accuracy: 0.7859\n",
      "Epoch 4/5\n",
      "1819/1819 [==============================] - 9s 5ms/step - loss: 0.3754 - binary_accuracy: 0.8451 - val_loss: 0.4562 - val_binary_accuracy: 0.7873\n",
      "Epoch 5/5\n",
      "1819/1819 [==============================] - 9s 5ms/step - loss: 0.3514 - binary_accuracy: 0.8558 - val_loss: 0.4565 - val_binary_accuracy: 0.7883\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\samue\\Documents\\Applied Data Science\\INFO-H518 Deep Learning\\Assignments\\A3\\Models\\M2\\assets\n"
     ]
    }
   ],
   "source": [
    "history = kr_words.fit(\n",
    "    training,\n",
    "    validation_data=validation,\n",
    "    epochs=epochs\n",
    ")\n",
    "kr_words.save(r'C:\\Users\\samue\\Documents\\Applied Data Science\\INFO-H518 Deep Learning\\Assignments\\A3\\Models\\M2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M2 | Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429/429 [==============================] - 1s 2ms/step - loss: 0.4388 - binary_accuracy: 0.8003\n",
      "Loss: 0.4388076663017273 \n",
      "Accuracy: 0.800305962562561\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = kr_words.evaluate(testing)\n",
    "print(f\"Loss: {loss} \\nAccuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epochs = 5 \n",
    "\n",
    "K = 25 \n",
    "\n",
    "Layers = [embedding | globalAvgPooling1D | dense(16, activation='swish', regularizer='l2') | dense(1)]\n",
    "\n",
    "Embedding_dim = 32 \n",
    "\n",
    "Optimizer = 'adam' (standard settings)\n",
    "\n",
    "Loss = BinaryCrossEntropy : 0.4388\n",
    "\n",
    "Accuracy = 0.8003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model #3: K-words with RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M3 | Create the structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "epochs = 5\n",
    "embedding_dim = 32\n",
    "\n",
    "# Structure\n",
    "handle_kr_words = Sequential([\n",
    "    layers.Embedding(vocab_size, embedding_dim),\n",
    "    layers.GlobalAveragePooling1D(),\n",
    "    layers.Dense(8, activation='swish', kernel_regularizer='l2'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "handle_kr_words.compile(\n",
    "    loss=losses.BinaryCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.RMSprop(\n",
    "        learning_rate=0.0012,\n",
    "        rho=0.999,\n",
    "        momentum=0.0,\n",
    "        epsilon=1e-07,\n",
    "        centered=False,\n",
    "        name='RMSprop'\n",
    "    ),\n",
    "    metrics=tf.metrics.BinaryAccuracy(threshold=0.0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M3 | Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1819/1819 [==============================] - 10s 5ms/step - loss: 0.5672 - binary_accuracy: 0.7173 - val_loss: 0.5065 - val_binary_accuracy: 0.7567\n",
      "Epoch 2/5\n",
      "1819/1819 [==============================] - 9s 5ms/step - loss: 0.4414 - binary_accuracy: 0.8072 - val_loss: 0.4719 - val_binary_accuracy: 0.7720\n",
      "Epoch 3/5\n",
      "1819/1819 [==============================] - 9s 5ms/step - loss: 0.3931 - binary_accuracy: 0.8321 - val_loss: 0.4600 - val_binary_accuracy: 0.7764\n",
      "Epoch 4/5\n",
      "1819/1819 [==============================] - 9s 5ms/step - loss: 0.3635 - binary_accuracy: 0.8478 - val_loss: 0.4573 - val_binary_accuracy: 0.7809\n",
      "Epoch 5/5\n",
      "1819/1819 [==============================] - 10s 5ms/step - loss: 0.3424 - binary_accuracy: 0.8579 - val_loss: 0.4593 - val_binary_accuracy: 0.7812\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\samue\\Documents\\Applied Data Science\\INFO-H518 Deep Learning\\Assignments\\A3\\Models\\M3\\assets\n"
     ]
    }
   ],
   "source": [
    "history = handle_kr_words.fit(\n",
    "    training,\n",
    "    validation_data=validation,\n",
    "    epochs=epochs\n",
    ")\n",
    "handle_kr_words.save(r'C:\\Users\\samue\\Documents\\Applied Data Science\\INFO-H518 Deep Learning\\Assignments\\A3\\Models\\M3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M3 | Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429/429 [==============================] - 1s 3ms/step - loss: 0.4420 - binary_accuracy: 0.7969\n",
      "Loss: 0.442028284072876 \n",
      "Accuracy: 0.7968818545341492\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = handle_kr_words.evaluate(testing)\n",
    "print(f\"Loss: {loss} \\nAccuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epochs = 5 \n",
    "\n",
    "K = 25\n",
    "\n",
    "Layers = [embedding | globalAvgPooling1D | dense(8, activation='swish', regularizer='l2') | dense(1)]\n",
    "\n",
    "Embedding_dim = 32 \n",
    "\n",
    "Optimizer = RMSProp\n",
    "\n",
    "Loss = .4420\n",
    "\n",
    "Accuracy = 0.7969"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model #4: K-words with Handle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Retrieval | Input Pipeline | Encode the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet:  b'RepScottPeters #TeamPeters met with @FamilyHealthSD and Essential Health Access to disucss the impotance of Title X funding. I\\xe2\\x80\\x99m p\\xe2\\x80\\xa6 https://t.co/6YwN9NcSOp'\n",
      "Label:  0\n",
      "Encoded Tweet:  (<tf.Tensor: shape=(1, 25), dtype=int64, numpy=\n",
      "array([[ 1099,  6280,   268,    14,     1,     5, 22393,   403,  2494,\n",
      "            3,     1,     2,     1,     4,  4474,  4448,   184,    10,\n",
      "           48,   393,     0,     0,     0,     0,     0]], dtype=int64)>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n"
     ]
    }
   ],
   "source": [
    "# DATA RETRIEVAL\n",
    "\n",
    "def with_handle(x):\n",
    "    x['Tweet'] = f\"{x['Handle']} \" + x['Tweet']\n",
    "    return x\n",
    "train_ = train.apply(with_handle, 1)\n",
    "valid_ = valid.apply(with_handle, 1)\n",
    "test_ = test.apply(with_handle, 1)\n",
    "\n",
    "vocab_size = train_vocab['Terms'] \\\n",
    "    .append(test_vocab['Terms']) \\\n",
    "    .append(pd.DataFrame(train_.Handle.unique(), columns=['Terms'])['Terms']) \\\n",
    "    .append(pd.DataFrame(valid_.Handle.unique(), columns=['Terms'])['Terms']) \\\n",
    "    .append(pd.DataFrame(test_.Handle.unique(), columns=['Terms'])['Terms']) \\\n",
    "    .unique().shape[0]\n",
    "\n",
    "# Format into Tensors\n",
    "train_dataset = tf.data.Dataset \\\n",
    "    .from_tensor_slices(\n",
    "        (train_['Tweet'].to_numpy(), train_['Party'].to_numpy())\n",
    "    )\n",
    "\n",
    "valid_dataset = tf.data.Dataset \\\n",
    "    .from_tensor_slices(\n",
    "        (valid_['Tweet'].to_numpy(), valid_['Party'].to_numpy())\n",
    "    )\n",
    "\n",
    "test_dataset = tf.data.Dataset \\\n",
    "    .from_tensor_slices(\n",
    "        (test_['Tweet'].to_numpy(), test_['Party'].to_numpy())\n",
    "    )\n",
    "\n",
    "\n",
    "# INPUT PIPELINE\n",
    "\n",
    "# Set up the input pipeline\n",
    "BUFFER_SIZE = 5000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = train_dataset \\\n",
    "    .shuffle(BUFFER_SIZE) \\\n",
    "    .batch(BATCH_SIZE)\n",
    "\n",
    "valid_dataset = valid_dataset \\\n",
    "    .shuffle(BUFFER_SIZE) \\\n",
    "    .batch(BATCH_SIZE)\n",
    "\n",
    "test_dataset = test_dataset \\\n",
    "    .shuffle(BUFFER_SIZE) \\\n",
    "    .batch(BATCH_SIZE)\n",
    "\n",
    "\n",
    "# ENCODE THE DATA\n",
    "\n",
    "# Parameters\n",
    "k = 25\n",
    "\n",
    "# Vectorization Layer\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize=cleaned_data,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=k\n",
    ")\n",
    "\n",
    "train_features = train_dataset.map(lambda x, y: x)\n",
    "valid_features = valid_dataset.map(lambda x, y: x)\n",
    "test_features = test_dataset.map(lambda x, y: x) \n",
    "\n",
    "vectorize_layer.adapt(train_features)\n",
    "vectorize_layer.adapt(valid_features)\n",
    "vectorize_layer.adapt(test_features)\n",
    "\n",
    "def vectorize_me(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text), label\n",
    "\n",
    "text_batch, label_batch = next(iter(train_dataset))\n",
    "first_review, first_label = text_batch[0], label_batch[0]\n",
    "print(\"Tweet: \", first_review.numpy())\n",
    "print(\"Label: \", first_label.numpy())\n",
    "print(\"Encoded Tweet: \", vectorize_me(first_review, first_label))\n",
    "\n",
    "training = train_dataset.map(vectorize_me).cache().prefetch(tf.data.AUTOTUNE)\n",
    "validation = valid_dataset.map(vectorize_me).prefetch(tf.data.AUTOTUNE)\n",
    "testing = test_dataset.map(vectorize_me).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure\n",
    "embedding_dim = 32\n",
    "epochs = 5\n",
    "\n",
    "handle_with_k_words = Sequential([\n",
    "    layers.Embedding(vocab_size, embedding_dim),\n",
    "    layers.GlobalAveragePooling1D(),\n",
    "    layers.Dense(16, activation='swish', kernel_regularizer='l2'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "handle_with_k_words.compile(\n",
    "    loss=losses.BinaryCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.Adam(\n",
    "        learning_rate=0.001,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-07,\n",
    "        amsgrad=False,\n",
    "        name='Adam'\n",
    "    ),\n",
    "    metrics=tf.metrics.BinaryAccuracy(threshold=0.0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1819/1819 [==============================] - 10s 5ms/step - loss: 0.4295 - binary_accuracy: 0.8712 - val_loss: 0.1789 - val_binary_accuracy: 0.9888\n",
      "Epoch 2/5\n",
      "1819/1819 [==============================] - 9s 5ms/step - loss: 0.1146 - binary_accuracy: 0.9937 - val_loss: 0.0890 - val_binary_accuracy: 0.9933\n",
      "Epoch 3/5\n",
      "1819/1819 [==============================] - 9s 5ms/step - loss: 0.0637 - binary_accuracy: 0.9964 - val_loss: 0.0614 - val_binary_accuracy: 0.9938\n",
      "Epoch 4/5\n",
      "1819/1819 [==============================] - 9s 5ms/step - loss: 0.0441 - binary_accuracy: 0.9979 - val_loss: 0.0487 - val_binary_accuracy: 0.9940\n",
      "Epoch 5/5\n",
      "1819/1819 [==============================] - 9s 5ms/step - loss: 0.0337 - binary_accuracy: 0.9985 - val_loss: 0.0417 - val_binary_accuracy: 0.9941\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\samue\\Documents\\Applied Data Science\\INFO-H518 Deep Learning\\Assignments\\A3\\Models\\M4\\assets\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "history = handle_with_k_words.fit(\n",
    "    training,\n",
    "    validation_data=validation,\n",
    "    epochs=epochs\n",
    ")\n",
    "handle_with_k_words.save(r'C:\\Users\\samue\\Documents\\Applied Data Science\\INFO-H518 Deep Learning\\Assignments\\A3\\Models\\M4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429/429 [==============================] - 1s 2ms/step - loss: 0.0359 - binary_accuracy: 0.9964\n",
      "Loss: 0.035931963473558426 \n",
      "Accuracy: 0.9964301586151123\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = handle_with_k_words.evaluate(testing)\n",
    "print(f\"Loss: {loss} \\nAccuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recapitulation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the associated political party of a user based only on the content of the user's tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M1:\n",
    "\n",
    "Epochs = 5 \n",
    "\n",
    "K = 25\n",
    "\n",
    "Layers = [embedding | dropout(0.1) | globalAvgPooling1D | dropout(0.3) | dense(1)]\n",
    "\n",
    "Embedding_dim = 32 \n",
    "\n",
    "Optimizer = 'adam' (standard settings)\n",
    "\n",
    "Loss = .4155\n",
    "\n",
    "Accuracy = 0.8021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M2:\n",
    "\n",
    "Epochs = 5 \n",
    "\n",
    "K = 25 \n",
    "\n",
    "Layers = [embedding | globalAvgPooling1D | dense(16, activation='swish', regularizer='l2') | dense(1)]\n",
    "\n",
    "Embedding_dim = 32 \n",
    "\n",
    "Optimizer = 'adam' (standard settings)\n",
    "\n",
    "Loss = BinaryCrossEntropy : 0.4388\n",
    "\n",
    "Accuracy = 0.8003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M3:\n",
    "\n",
    "Epochs = 5 \n",
    "\n",
    "K = 25\n",
    "\n",
    "Layers = [embedding | globalAvgPooling1D | dense(8, activation='swish', regularizer='l2') | dense(1)]\n",
    "\n",
    "Embedding_dim = 32 \n",
    "\n",
    "Optimizer = RMSProp\n",
    "\n",
    "Loss = .4420\n",
    "\n",
    "Accuracy = 0.7969"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the associated political party of a user based on the user's Handle and the content of the user's tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M4:\n",
    "Use K words PLUS the user's Handle from a tweet to predict the user's political party.\n",
    "\n",
    "Epochs = 5 \n",
    "\n",
    "K = 25 \n",
    "\n",
    "Layers = [embedding | globalAvgPooling1D | dense(16, activation='swish', regularizer='l2') | dense(1)]\n",
    "\n",
    "Embedding_dim = 32 \n",
    "\n",
    "Optimizer = 'adam' (standard settings)\n",
    "\n",
    "Loss = 0.0359\n",
    "\n",
    "Accuracy = 0.9964\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9a9f9d2636d429c0263fe515000376bf2e0f1a96bb6efde409a9a507504e7fc3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
