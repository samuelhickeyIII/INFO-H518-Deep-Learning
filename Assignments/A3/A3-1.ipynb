{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdd76e9e",
   "metadata": {},
   "source": [
    "#### Data description\n",
    "This dataset has three columns - label (party name), twitter handle, tweet text\n",
    "\n",
    "\n",
    "#### Problem Description:\n",
    "\n",
    "Design a feed forward deep neural network to predict the political party using the pytorch or tensorflow. \n",
    "Build two models\n",
    "\n",
    "1. Without using the handle\n",
    "\n",
    "2. Using the handle\n",
    "\n",
    "\n",
    "#### Deliverables\n",
    "\n",
    "- Report the performance on the test set.\n",
    "\n",
    "- Try multiple models and with different hyperparameters. Present the results of each model on the test set. No need to create a dev set.\n",
    "\n",
    "- Experiment with:\n",
    "    -L2 and dropout regularization techniques\n",
    "    -SGD, RMSProp and Adamp optimization techniques\n",
    "\n",
    "\n",
    "\n",
    "- Creating a fixed-sized vocabulary: Give a unique id to each word in your selected vocabulary and use it as the input to the network\n",
    "\n",
    "    - Option 1: Feedforward networks can only handle fixed-sized inputs. You can choose to have a fixed-sized K words from the tweet text (e.g. the first K word, randomly selected K word etc.). K can be a hyperparameter. \n",
    "\n",
    "    - Option 2: you can choose top N (e.g. N=1000) frequent words from the dataset and use an N-sized input layer. If a word is present in a tweet, pass the id, 0 otherwise\n",
    "    \n",
    "    -  Clearly state your design choices and assumptions. Think about the pros and cons of each option.\n",
    "\n",
    " \n",
    "\n",
    "<b> Tabulate your results, either at the end of the code file or in the text box on the submission page. The final result should have:</b>\n",
    "\n",
    "1. Experiment description\n",
    "\n",
    "2. Hyperparameter used and their values\n",
    "\n",
    "3. Performance on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2df7de78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, losses\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import text\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "PATH = r\"C:\\Users\\samue\\Documents\\Applied Data Science\\INFO-H518 Deep Learning\\Assignments\\A3\\Input\"\n",
    "\n",
    "train = pd.read_pickle(PATH + r'\\train_tokenized.pickle').dropna().sample(frac=1)\n",
    "train_vocab = pd.read_csv(PATH + r'\\train_vocab.csv', header=0).dropna()\n",
    "test = pd.read_pickle(PATH + r'\\test_tokenized.pickle').dropna().sample(frac=1)\n",
    "test_vocab = pd.read_csv(PATH + r'\\test_vocab.csv', header=0).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f45912a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "sequence_length = 250\n",
    "\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    max_tokens=max_features,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23728cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ = vectorize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
