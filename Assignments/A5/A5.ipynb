{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author: Samuel Hickey\n",
    "### Assignment 5 - Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Problem Statement 1</center>\n",
    "\n",
    "<center>Build a sequential model to classify names into gender. </center>\n",
    "<center>Input to the model will be a name, i.e. a sequence of characters. </center>\n",
    "<center>Use one hot representation of the characters. </center>\n",
    "<center>Remove non-ascii characters, if there are any</center>\n",
    "\n",
    "<center>Show the effect of the following on the accuracy:</center>\n",
    "<center>RNN Cells: SimpleRNN, LSTM, and GRU</center>\n",
    "<center>Dataset size: 25%, 50%, 75%, and 100% of the data (.8 to .2 split)</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, GRU, LSTM, SimpleRNN, Bidirectional\n",
    "\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "PATH = r\"C:\\Users\\samue\\Documents\\Applied Data Science\\INFO-H518 Deep Learning\\Assignments\\A5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data and shuffle it\n",
    "data = pd.read_csv(PATH+r\"\\data\\name_gender.csv\").dropna().sample(frac=1.0)\n",
    "max_len = data['name'].map(lambda x: len(x)).max()\n",
    "EPOCHS = 15\n",
    "VERBOSE = 1\n",
    "optimizer = tf.keras.optimizers.Adam(clipvalue=.3)\n",
    "\n",
    "data['gender'] = data.gender.map({'M': 0, 'F': 1})\n",
    "data['M'] = data.gender.map({0:1, 1:0})\n",
    "data['F'] = data.gender.map({0:0, 1:1})\n",
    "\n",
    "# Remove non-ascii characters from names\n",
    "names = data['name'].replace({r'[^\\x00-\\x7F]+':''}, regex=True)\n",
    "name_chars = tf.strings.unicode_split(names.to_numpy(str), 'UTF-8')\n",
    "\n",
    "# Create the model's vocabulary\n",
    "letters = []\n",
    "for i, name in names.iteritems():\n",
    "    letters = list(set(list(set(letters)) + list(set(name))))\n",
    "o_letters = letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_from_chars = keras.layers.StringLookup(vocabulary=letters, mask_token=None)\n",
    "ids_from_chars_o = ids_from_chars\n",
    "chars_from_ids = tf.keras.layers.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
    "\n",
    "# # Pad each name, new shape: (1 x max_len) \n",
    "# x, y = [], []\n",
    "# for i, row in data.iterrows():\n",
    "#     tmp = (max_len - len(row['name'])) * '0' + row['name']\n",
    "#     x.append(tmp)\n",
    "#     y.append([row['M']*row['probability'], row['F']*row['probability']])\n",
    "\n",
    "# # One Hot Encode the padded names\n",
    "# _x = np.zeros((len(x), max_len, len(letters)), dtype=bool)\n",
    "# _y = np.zeros((len(x), 2), dtype=bool)\n",
    "# for i, seq in enumerate(x):\n",
    "#     for j, c in enumerate(seq):\n",
    "#         if c != '0':\n",
    "#             _x[i, j, ids_from_chars(c).numpy()-1] = 1\n",
    "#     _y[i] = y[i]\n",
    "#     if i % 1000 == 0: print(i)\n",
    "\n",
    "# np.save(PATH+r'\\data\\x_names_w_single_class_shuffled', _x)\n",
    "# np.save(PATH+r'\\data\\y_names_w_single_class_shuffled', _y)\n",
    "\n",
    "train_x = np.load(PATH+r'\\data\\x_names_w_single_class_shuffled.npy')\n",
    "train_y = np.load(PATH+r'\\data\\y_names_w_single_class_shuffled.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(x, y, split):\n",
    "    index = round(x.shape[0]*split)\n",
    "\n",
    "    x_, y_ = x[:index].astype('float32'), y[:index].astype('float32')\n",
    "\n",
    "    index = round(x_.shape[0]*.8)\n",
    "\n",
    "    train_x_, train_y_ = x_[:index], y_[:index]\n",
    "    test_x_, test_y_ = x_[index:], y_[index:]\n",
    "    return (train_x_, train_y_, test_x_, test_y_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Size: 25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19005, 15, 52) (19005, 2)\n"
     ]
    }
   ],
   "source": [
    "train_x_, train_y_, test_x_, test_y_ = split(train_x, train_y, .25)\n",
    "print(train_x_.shape, train_y_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s_25 = keras.models.Sequential([\n",
    "#     Bidirectional(SimpleRNN(64, input_shape=(max_len, len(letters)))),\n",
    "#     Dense(2, activation='softmax')\n",
    "# ])\n",
    "# s_25.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "# s_25.fit(train_x_, train_y_, epochs=EPOCHS, validation_split=.2, verbose=VERBOSE)\n",
    "# s_25.save(PATH+r'\\models\\srnn\\twenty_five')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149/149 [==============================] - 2s 7ms/step - loss: 0.3455 - acc: 0.8615\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3454729914665222, 0.8615028262138367]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_25 = keras.models.load_model(PATH+r'\\models\\srnn\\twenty_five', custom_objects={'optimizer':optimizer})\n",
    "s_25.evaluate(test_x_, test_y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm_25 = keras.models.Sequential([\n",
    "#     Bidirectional(LSTM(64, input_shape=(max_len, len(letters)))),\n",
    "#     Dense(2, activation='softmax')\n",
    "# ])\n",
    "# lstm_25.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "# lstm_25.fit(train_x_, train_y_, epochs=EPOCHS, validation_split=.2, verbose=VERBOSE)\n",
    "# lstm_25.save(PATH+r'\\models\\lstm\\twenty_five')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149/149 [==============================] - 2s 5ms/step - loss: 0.3239 - acc: 0.8695\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.32392430305480957, 0.8695011734962463]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_25 = keras.models.load_model(PATH+r'\\models\\lstm\\twenty_five', custom_objects={'optimizer':optimizer})\n",
    "lstm_25.evaluate(test_x_, test_y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gru_25 = keras.models.Sequential([\n",
    "#     Bidirectional(GRU(64, input_shape=(max_len, len(letters)))),\n",
    "#     Dense(2, activation='softmax')\n",
    "# ])\n",
    "# gru_25.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "# gru_25.fit(train_x_, train_y_, epochs=EPOCHS, validation_split=.2, verbose=VERBOSE)\n",
    "# gru_25.save(PATH+r'\\models\\gru\\twenty_five')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149/149 [==============================] - 1s 5ms/step - loss: 0.3262 - acc: 0.8724\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3261739909648895, 0.8724479079246521]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru_25 = keras.models.load_model(PATH+r'\\models\\gru\\twenty_five', custom_objects={'optimizer':optimizer})\n",
    "gru_25.evaluate(test_x_, test_y_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Size: 50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38010, 15, 52)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_, train_y_, test_x_, test_y_ = split(train_x, train_y, .5)\n",
    "train_x_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s_50 = keras.models.Sequential([\n",
    "#     Bidirectional(SimpleRNN(64, input_shape=(max_len, len(letters)))),\n",
    "#     Dense(2, activation='softmax')\n",
    "# ])\n",
    "# s_50.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "# s_50.fit(train_x_, train_y_, epochs=EPOCHS, validation_split=.2, verbose=VERBOSE)\n",
    "# s_50.save(PATH+r'\\models\\srnn\\fifty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297/297 [==============================] - 2s 7ms/step - loss: 0.3297 - acc: 0.8639\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3296985924243927, 0.8639233708381653]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_50 = keras.models.load_model(PATH+r'\\models\\srnn\\fifty', custom_objects={'optimizer':optimizer})\n",
    "s_50.evaluate(test_x_, test_y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm_50 = keras.models.Sequential([\n",
    "#     Bidirectional(LSTM(64, input_shape=(max_len, len(letters)))),\n",
    "#     Dense(2, activation='softmax')\n",
    "# ])\n",
    "# lstm_50.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "# lstm_50.fit(train_x_, train_y_, epochs=EPOCHS, validation_split=.2, verbose=VERBOSE)\n",
    "# lstm_50.save(PATH+r'\\models\\lstm\\fifty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297/297 [==============================] - 2s 5ms/step - loss: 0.2993 - acc: 0.8801\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2992900311946869, 0.8801304697990417]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_50 = keras.models.load_model(PATH+r'\\models\\lstm\\fifty', custom_objects={'optimizer':optimizer})\n",
    "lstm_50.evaluate(test_x_, test_y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gru_50 = keras.models.Sequential([\n",
    "#     Bidirectional(GRU(64, input_shape=(max_len, len(letters)))),\n",
    "#     Dense(2, activation='softmax')\n",
    "# ])\n",
    "# gru_50.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "# gru_50.fit(train_x_, train_y_, epochs=EPOCHS, validation_split=.2, verbose=VERBOSE)\n",
    "# gru_50.save(PATH+r'\\models\\gru\\fifty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297/297 [==============================] - 2s 5ms/step - loss: 0.3037 - acc: 0.8832\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.30372515320777893, 0.8831824660301208]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru_50 = keras.models.load_model(PATH+r'\\models\\gru\\fifty', custom_objects={'optimizer':optimizer})\n",
    "gru_50.evaluate(test_x_, test_y_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Size: 75%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57015, 15, 52)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_, train_y_, test_x_, test_y_ = split(train_x, train_y, .75)\n",
    "train_x_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s_75 = keras.models.Sequential([\n",
    "#     Bidirectional(SimpleRNN(64, input_shape=(max_len, len(letters)))),\n",
    "#     Dense(2, activation='softmax')\n",
    "# ])\n",
    "# s_75.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "# s_75.fit(train_x_, train_y_, epochs=EPOCHS, validation_split=.2, verbose=VERBOSE)\n",
    "# s_75.save(PATH+r'\\models\\srnn\\seventy_five')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "446/446 [==============================] - 3s 7ms/step - loss: 0.3056 - acc: 0.8774\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.30558502674102783, 0.8774378895759583]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_75 = keras.models.load_model(PATH+r'\\models\\srnn\\seventy_five', custom_objects={'optimizer':optimizer})\n",
    "s_75.evaluate(test_x_, test_y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm_75 = keras.models.Sequential([\n",
    "#     Bidirectional(LSTM(64, input_shape=(max_len, len(letters)))),\n",
    "#     Dense(2, activation='softmax')\n",
    "# ])\n",
    "# lstm_75.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "# lstm_75.fit(train_x_, train_y_, epochs=EPOCHS, validation_split=.2, verbose=VERBOSE)\n",
    "# lstm_75.save(PATH+r'\\models\\lstm\\seventy_five')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "446/446 [==============================] - 3s 5ms/step - loss: 0.2729 - acc: 0.8915\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.27294453978538513, 0.8914690613746643]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_75 = keras.models.load_model(PATH+r'\\models\\lstm\\seventy_five', custom_objects={'optimizer':optimizer})\n",
    "lstm_75.evaluate(test_x_, test_y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gru_75 = keras.models.Sequential([\n",
    "#     Bidirectional(GRU(64, input_shape=(max_len, len(letters)))),\n",
    "#     Dense(2, activation='softmax')\n",
    "# ])\n",
    "# gru_75.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "# gru_75.fit(train_x_, train_y_, epochs=EPOCHS, validation_split=.2, verbose=VERBOSE)\n",
    "# gru_75.save(PATH+r'\\models\\gru\\seventy_five')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "446/446 [==============================] - 3s 5ms/step - loss: 0.2871 - acc: 0.8905\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.28710517287254333, 0.8904868960380554]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru_75 = keras.models.load_model(PATH+r'\\models\\gru\\seventy_five', custom_objects={'optimizer':optimizer})\n",
    "gru_75.evaluate(test_x_, test_y_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Size: 100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76020, 15, 52)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_, train_y_, test_x_, test_y_ = split(train_x, train_y, 1.0)\n",
    "train_x_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s_100 = keras.models.Sequential([\n",
    "#     Bidirectional(SimpleRNN(64, input_shape=(max_len, len(letters)))),\n",
    "#     Dense(2, activation='softmax')\n",
    "# ])\n",
    "# s_100.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "# s_100.fit(train_x_, train_y_, epochs=EPOCHS, validation_split=.2, verbose=VERBOSE)\n",
    "# s_100.save(PATH+r'\\models\\srnn\\all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "594/594 [==============================] - 4s 7ms/step - loss: 0.2889 - acc: 0.8811\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.28891894221305847, 0.8811365365982056]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_100 = keras.models.load_model(PATH+r'\\models\\srnn\\all', custom_objects={'optimizer':optimizer})\n",
    "s_100.evaluate(test_x_, test_y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm_100 = keras.models.Sequential([\n",
    "#     Bidirectional(LSTM(64, input_shape=(max_len, len(letters)))),\n",
    "#     Dense(2, activation='softmax')\n",
    "# ])\n",
    "# lstm_100.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "# lstm_100.fit(train_x_, train_y_, epochs=EPOCHS, validation_split=.2, verbose=VERBOSE)\n",
    "# lstm_100.save(PATH+r'\\models\\lstm\\all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "594/594 [==============================] - 4s 5ms/step - loss: 0.2562 - acc: 0.8981\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.256156861782074, 0.8981320858001709]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_100 = keras.models.load_model(PATH+r'\\models\\lstm\\all', custom_objects={'optimizer':optimizer})\n",
    "lstm_100.evaluate(test_x_, test_y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gru_100 = keras.models.Sequential([\n",
    "#     Bidirectional(GRU(64, input_shape=(max_len, len(letters)))),\n",
    "#     Dense(2, activation='softmax')\n",
    "# ])\n",
    "# gru_100.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "# gru_100.fit(train_x_, train_y_, epochs=EPOCHS, validation_split=.2, verbose=VERBOSE)\n",
    "# gru_100.save(PATH+r'\\models\\gru\\all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "594/594 [==============================] - 3s 5ms/step - loss: 0.2620 - acc: 0.8997\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.26202625036239624, 0.8997105956077576]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru_100 = keras.models.load_model(PATH+r'\\models\\gru\\all', custom_objects={'optimizer':optimizer})\n",
    "gru_100.evaluate(test_x_, test_y_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "\n",
    "Train a language model using these names, generate 100 male and female names, compare the accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create padded sequence tensors, each of the size (1 x max_len) for each name\n",
    "# the associated labels represent the next character in the sequence\n",
    "x, y = [], []\n",
    "for word in names:\n",
    "    tmp = (max_len - len(word)) * '0' + word + '\\n'\n",
    "    x.append(tmp[2:])\n",
    "    y.append('')\n",
    "    for i, j in enumerate(word):\n",
    "        if (i >= len(word) - 1):\n",
    "            break\n",
    "        tmp = (max_len - len(word[:-1-i])) * '0' + word[:-1-i].lower()\n",
    "        x.append(tmp)\n",
    "        y.append(word[-1-i])\n",
    "\n",
    "letters = list(set(list(set([i.lower() for i in letters])) + ['\\n']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_from_chars = keras.layers.StringLookup(vocabulary=letters, mask_token=None)\n",
    "chars_from_ids = tf.keras.layers.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
    "\n",
    "# # One Hot Encode the sequences\n",
    "# _x = np.zeros((len(x), max_len, len(letters)), dtype=bool)\n",
    "# _y = np.zeros((len(x), len(letters)), dtype=bool)\n",
    "# for i, seq in enumerate(x):\n",
    "#     for j, c in enumerate(seq):\n",
    "#         if c != '0':\n",
    "#             _x[i, j, ids_from_chars(c).numpy()-1] = 1\n",
    "#     _y[i, ids_from_chars(y[i]).numpy()-1] = 1\n",
    "#     if i % round(.1*_x.shape[0]) == 0: print()\n",
    "\n",
    "# np.save(PATH+r'\\data\\x_names_shuffled_lower_stop', _x)\n",
    "# np.save(PATH+r'\\data\\y_names_shuffled_lower_stop', _y)\n",
    "\n",
    "train_x = np.load(PATH+r'\\data\\x_names_shuffled_lower_stop.npy')\n",
    "train_y = np.load(PATH+r'\\data\\y_names_shuffled_lower_stop.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lang_mod = keras.models.Sequential([\n",
    "#     Bidirectional(GRU(64, input_shape=(max_len, len(letters)))),\n",
    "#     Dense(len(letters), activation='softmax')\n",
    "# ])\n",
    "# lang_mod.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "# lang_mod.fit(train_x.astype('float32'), train_y.astype('float32'), epochs=15, validation_split=.2, verbose=1)\n",
    "# lang_mod.save(PATH+r'\\models\\lang_mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_mod = keras.models.load_model(PATH+r'\\models\\lang_mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['N', 'M', 'X', 'B', 'G', 'J', 'G', 'T', 'I', 'J', 'V', 'K', 'A', 'V', 'Q', 'W', 'N', 'D', 'B', 'Y', 'M', 'D', 'F', 'R', 'A', 'J', 'L', 'O', 'E', 'E', 'I', 'Y', 'F', 'N', 'J', 'P', 'B', 'E', 'V', 'I', 'H', 'C', 'I', 'U', 'X', 'Q', 'L', 'B', 'C', 'V', 'M', 'T', 'N', 'E', 'K', 'A', 'S', 'X', 'E', 'M', 'L', 'A', 'X', 'V', 'M', 'C', 'K', 'J', 'W', 'S', 'X', 'L', 'W', 'C', 'R', 'J', 'Y', 'D', 'B', 'F', 'R', 'Q', 'P', 'P', 'H', 'F', 'E', 'M', 'N', 'W', 'C', 'J', 'X', 'H', 'E', 'Y', 'Y', 'W', 'B', 'G']\n",
      "Males: 83 | Females: 0\n",
      "Males: 100 | Females: 67\n",
      "Males: 100 | Females: 100\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "base = [np.random.choice(list(string.ascii_uppercase)) for _ in range(100)]\n",
    "print(base)\n",
    "dist = [int(i) for i in np.random.normal(6, 2, 100)]\n",
    "males, females = [], []\n",
    "while len(males) < 100 or len(females) < 100:\n",
    "    for i in base:\n",
    "        seq = ('{0:0>' + str(max_len)+'}').format(i).lower()\n",
    "        new_word = i\n",
    "        name_len = np.random.choice(dist)\n",
    "        while (len(new_word) < max_len):\n",
    "            # Vectorize the input of the model.\n",
    "            x_pred = np.zeros((1, max_len, len(letters)))\n",
    "            for j, c in enumerate(seq):\n",
    "                if c != '0' and j < x_pred.shape[1]:\n",
    "                    x_pred[0, j, ids_from_chars(c)-1] = 1\n",
    "\n",
    "            # Predict the probabilities of the next char.\n",
    "            preds = lang_mod.predict(x_pred, verbose=0)[0]\n",
    "            preds = np.exp(preds) / np.sum(np.exp(preds))\n",
    "\n",
    "            # Randomly sample from the top ten percent of probabilities \n",
    "            top_ten_pct = []\n",
    "            for _ in range(np.ceil(len(preds)*.05).astype(np.int16)):\n",
    "                index = np.where(preds == max(preds))[0][0]\n",
    "                top_ten_pct.append(index)\n",
    "                preds[index] = 0.0\n",
    "            next_char = chars_from_ids(np.random.choice(top_ten_pct)+1) \\\n",
    "                .numpy().decode('utf-8')\n",
    "\n",
    "            if ((next_char == '\\n'\n",
    "                    or next_char in string.ascii_uppercase\n",
    "                    or len(new_word) > name_len) and len(new_word) > 1):\n",
    "                break\n",
    "            else:\n",
    "                # Append the character\n",
    "                new_word += next_char.lower()\n",
    "                # Add pre-padding of zeros to the sequence generated and continue.\n",
    "                seq = ('{0:0>' + str(max_len) + '}').format(new_word).lower()\n",
    "        tmp = (max_len - len(new_word)) * '0' + new_word\n",
    "        new_vec = np.zeros((1, max_len, len(o_letters)))\n",
    "        for k, c in enumerate(tmp):\n",
    "            new_vec[0, k, ids_from_chars_o(c).numpy()-1] = 1\n",
    "        idx = list(gru_100.predict(new_vec, verbose=0)[0])\n",
    "        if idx.index(max(idx)) == 0 and len(males) < 100:\n",
    "            males.append(new_word)\n",
    "        elif idx.index(max(idx)) == 0 and len(females) < 100:\n",
    "            females.append(new_word)\n",
    "    print(f\"Males: {len(males)} | Females: {len(females)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Males: ['Malekai', 'Xilandrasia', 'Gabria', 'Gabria', 'Ilisa', 'Jame', 'Vanella', 'Karille', 'Alisan', 'Qaileney', 'Waldardand', 'Brayl', 'Yasi', 'Malan', 'Demerica', 'Falis', 'Raylee', 'Alexi', 'Latrenish', 'Orlineth', 'Elissiaha', 'Emi', 'Ilandra', 'Yarish', 'Freddic', 'Nathaleerah', 'Jerima', 'Pettya', 'Bertina', 'Ellie', 'Verlien', 'Ilisanneralded', 'Hellena', 'Chanteli', 'Ilis', 'Urellan', 'Xilar', 'Qaarisha', 'Carmelle', 'Vela', 'Margarielan', 'Taneisha', 'Nathaliner', 'Elissande', 'Karlee', 'Alian', 'Salimar', 'Xaivien', 'Ellis', 'Malekain', 'Lasha', 'Annes', 'Xila', 'Mikay', 'Kendrel', 'Jameichaella', 'Win', 'Shelli', 'Lassan', 'Wald', 'Channethean', 'Roselle', 'Jessi', 'Yarelis', 'Darlisah', 'Brannel', 'Freedicheekw', 'Ros', 'Qaatielia', 'Patreli', 'Paritta', 'Helliea', 'Elizethe', 'Nickol', 'Winteriou', 'Carlieg', 'Jessell', 'Xaivienet', 'Helianah', 'Eliza', 'Yosef', 'Wilmanig', 'Brendanne', 'Na', 'Mikeiahia', 'Xiara', 'Genevi', 'Tenishaw', 'Ili', 'Jerem', 'Vanil', 'Kendric', 'Alexa', 'Verlenad', 'Quitavaus', 'Win', 'Nichae', 'Demetar', 'Bentellah', 'Ya']\n",
      "Females: ['Daman', 'Falish', 'Ranell', 'Alissah', 'Jamar', 'Leahnne', 'Oline', 'Elle', 'Elisabalatasha', 'Ilia', 'Freedynetha', 'Nikos', 'Jestiel', 'Berthel', 'Emmaria', 'Ishan', 'Herli', 'Cherica', 'Isal', 'Uliam', 'Xil', 'Quin', 'Brently', 'Charm', 'Mikeila', 'Ta', 'Natha', 'Elisahna', 'Karlieg', 'Anarael', 'Sali', 'Xa', 'Emilia', 'Milinah', 'Leile', 'Alisseah', 'Vanes', 'Mal', 'Car', 'Jameic', 'Wa', 'Samilahianaham', 'Xairel', 'Lassielan', 'Waria', 'Carmeli', 'Roneill', 'Jest', 'Yasia', 'Demarcosend', 'Fariann', 'Rayl', 'Quitav', 'Perithannie', 'Herm', 'Fari', 'Milanah', 'Nathale', 'Wally', 'Chelen', 'Jeric', 'Xaveri', 'Hermal', 'Elizettera', 'Yaristiaha', 'Branniell', 'Ga', 'Nariyellan', 'Miliahn', 'Xa', 'Breal', 'Ja', 'Garrell', 'Ilarahn', 'Jam', 'Velaniah', 'Kenna', 'Velinam', 'Wardel', 'Yarel', 'Marial', 'Damielaha', 'Falemic', 'Ranel', 'Aliah', 'Jer', 'Leahna', 'Orlieg', 'Emmeri', 'Elisahra', 'Ilar', 'Yorena', 'Farr', 'Nickal', 'Jessell', 'Pa', 'Emmaly', 'Veland', 'Iland', 'Hale']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Males: {males}\")\n",
    "print(f\"Females: {females}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MALES\n",
    "# Pad each name, new shape: (1 x max_len) \n",
    "males_x, males_y = [], []\n",
    "for i, word in enumerate(males):\n",
    "    tmp = (max_len - len(word)) * '0' + word\n",
    "    males_x.append(tmp)\n",
    "    males_y.append([1.0, 0.0])\n",
    "\n",
    "# One Hot Encode the padded names\n",
    "m_x = np.zeros((len(males_x), max_len, len(o_letters)), dtype=bool)\n",
    "m_y = np.zeros((len(males_x), 2), dtype=bool)\n",
    "for i, seq in enumerate(males_x):\n",
    "    for j, c in enumerate(seq):\n",
    "        if c != '0':\n",
    "            m_x[i, j, ids_from_chars(c).numpy()-1] = 1\n",
    "    m_y[i] = y[i]\n",
    "\n",
    "# FEMALES\n",
    "# Pad each name, new shape: (1 x max_len) \n",
    "females_x, females_y = [], []\n",
    "for i, word in enumerate(females):\n",
    "    tmp = (max_len - len(word)) * '0' + word\n",
    "    females_x.append(tmp)\n",
    "    females_y.append([0.0, 1.0])\n",
    "\n",
    "# One Hot Encode the padded names\n",
    "f_x = np.zeros((len(females_x), max_len, len(o_letters)), dtype=bool)\n",
    "f_y = np.zeros((len(females_x), 2), dtype=bool)\n",
    "for i, seq in enumerate(females_x):\n",
    "    for j, c in enumerate(seq):\n",
    "        if c != '0':\n",
    "            f_x[i, j, ids_from_chars(c).numpy()-1] = 1\n",
    "    f_y[i] = y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 5ms/step - loss: 3.1997 - acc: 0.9700\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 3.4163 - acc: 0.9900\n"
     ]
    }
   ],
   "source": [
    "m_score = lstm_100.evaluate(m_x, m_y)\n",
    "f_score = lstm_100.evaluate(f_x, f_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem #2.a\n",
    "Create a model trained on only names starting with A, M, or Z, generate 50 names, determine the quality of the names using perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create padded sequence tensors, each of the size (1 x max_len) for each name\n",
    "# the associated labels represent the next character in the sequence\n",
    "x, y = [], []\n",
    "for word in names:\n",
    "    if word[0] in 'AMZ':\n",
    "        tmp = (max_len - len(word)) * '0' + word + '\\n'\n",
    "        x.append(tmp[2:])\n",
    "        y.append('')\n",
    "        for i, j in enumerate(word):\n",
    "            if (i >= len(word) - 1):\n",
    "                break\n",
    "            tmp = (max_len - len(word[:-1-i])) * '0' + word[:-1-i].lower()\n",
    "            x.append(tmp)\n",
    "            y.append(word[-1-i])\n",
    "\n",
    "letters = list(set(list(set([i.lower() for i in letters])) + ['\\n']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_from_chars = keras.layers.StringLookup(vocabulary=letters, mask_token=None)\n",
    "chars_from_ids = tf.keras.layers.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
    "\n",
    "# # One Hot Encode the sequences\n",
    "# _x = np.zeros((len(x), max_len, len(letters)), dtype=bool)\n",
    "# _y = np.zeros((len(x), len(letters)), dtype=bool)\n",
    "# for i, seq in enumerate(x):\n",
    "#     for j, c in enumerate(seq):\n",
    "#         if c != '0':\n",
    "#             _x[i, j, ids_from_chars(c).numpy()-1] = 1\n",
    "#     _y[i, ids_from_chars(y[i]).numpy()-1] = 1\n",
    "#     if i % round(.1*_x.shape[0]) == 0: print(i)\n",
    "\n",
    "# np.save(PATH+r'\\data\\x_names_shuffled_lower_stop_amz', _x)\n",
    "# np.save(PATH+r'\\data\\y_names_shuffled_lower_stop_amz', _y)\n",
    "\n",
    "train_x = np.load(PATH+r'\\data\\x_names_shuffled_lower_stop_amz.npy')\n",
    "train_y = np.load(PATH+r'\\data\\y_names_shuffled_lower_stop_amz.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "3133/3133 [==============================] - 28s 8ms/step - loss: 1.9654 - val_loss: 1.8423\n",
      "Epoch 2/15\n",
      "3133/3133 [==============================] - 25s 8ms/step - loss: 1.8158 - val_loss: 1.7858\n",
      "Epoch 3/15\n",
      "3133/3133 [==============================] - 25s 8ms/step - loss: 1.7678 - val_loss: 1.7570\n",
      "Epoch 4/15\n",
      "3133/3133 [==============================] - 25s 8ms/step - loss: 1.7361 - val_loss: 1.7295\n",
      "Epoch 5/15\n",
      "3133/3133 [==============================] - 25s 8ms/step - loss: 1.7108 - val_loss: 1.7155\n",
      "Epoch 6/15\n",
      "3133/3133 [==============================] - 25s 8ms/step - loss: 1.6900 - val_loss: 1.7023\n",
      "Epoch 7/15\n",
      "3133/3133 [==============================] - 25s 8ms/step - loss: 1.6724 - val_loss: 1.6965\n",
      "Epoch 8/15\n",
      "3133/3133 [==============================] - 24s 8ms/step - loss: 1.6577 - val_loss: 1.6877\n",
      "Epoch 9/15\n",
      "3133/3133 [==============================] - 25s 8ms/step - loss: 1.6448 - val_loss: 1.6812\n",
      "Epoch 10/15\n",
      "3133/3133 [==============================] - 26s 8ms/step - loss: 1.6325 - val_loss: 1.6781\n",
      "Epoch 11/15\n",
      "3133/3133 [==============================] - 25s 8ms/step - loss: 1.6215 - val_loss: 1.6767\n",
      "Epoch 12/15\n",
      "3133/3133 [==============================] - 25s 8ms/step - loss: 1.6116 - val_loss: 1.6729\n",
      "Epoch 13/15\n",
      "3133/3133 [==============================] - 25s 8ms/step - loss: 1.6020 - val_loss: 1.6701\n",
      "Epoch 14/15\n",
      "3133/3133 [==============================] - 24s 8ms/step - loss: 1.5926 - val_loss: 1.6673\n",
      "Epoch 15/15\n",
      "3133/3133 [==============================] - 24s 8ms/step - loss: 1.5842 - val_loss: 1.6668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_31_layer_call_fn, gru_cell_31_layer_call_and_return_conditional_losses, gru_cell_32_layer_call_fn, gru_cell_32_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\samue\\Documents\\Applied Data Science\\INFO-H518 Deep Learning\\Assignments\\A5\\models\\amz_lang_mod\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\samue\\Documents\\Applied Data Science\\INFO-H518 Deep Learning\\Assignments\\A5\\models\\amz_lang_mod\\assets\n"
     ]
    }
   ],
   "source": [
    "amz_lang_mod = keras.models.Sequential([\n",
    "    Bidirectional(GRU(64, input_shape=(max_len, len(letters)))),\n",
    "    Dense(len(letters), activation='softmax')\n",
    "])\n",
    "amz_lang_mod.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "amz_lang_mod.fit(train_x.astype('float32'), train_y.astype('float32'), epochs=15, validation_split=.2, verbose=1)\n",
    "amz_lang_mod.save(PATH+r'\\models\\amz_lang_mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'Z', 'A', 'Z', 'M', 'M', 'M', 'A', 'Z', 'Z', 'A', 'A', 'Z', 'M', 'A', 'A', 'A', 'M', 'M', 'Z', 'A', 'Z', 'Z', 'Z', 'M', 'A', 'M', 'A', 'M', 'M', 'A', 'M', 'M', 'A', 'A', 'M', 'Z', 'M', 'Z', 'M', 'M', 'A', 'Z', 'Z', 'Z', 'Z', 'A', 'Z', 'A', 'Z']\n"
     ]
    }
   ],
   "source": [
    "base = [np.random.choice(list('AMZ')) for _ in range(50)]\n",
    "print(base)\n",
    "dist = [int(i) for i in np.random.normal(6, 2, 100)]\n",
    "results = []\n",
    "for i in base:\n",
    "    seq = ('{0:0>' + str(max_len)+'}').format(i).lower()\n",
    "    new_word = i\n",
    "    name_len = np.random.choice(dist)\n",
    "    while (len(new_word) < max_len):\n",
    "        # Vectorize the input of the model.\n",
    "        x_pred = np.zeros((1, max_len, len(letters)))\n",
    "        for j, c in enumerate(seq):\n",
    "            if c != '0' and j < x_pred.shape[1]:\n",
    "                x_pred[0, j, ids_from_chars(c)-1] = 1\n",
    "\n",
    "        # Predict the probabilities of the next char.\n",
    "        preds = lang_mod.predict(x_pred, verbose=0)[0]\n",
    "        preds = np.exp(preds) / np.sum(np.exp(preds))\n",
    "\n",
    "        # Randomly sample from the top ten percent of probabilities \n",
    "        top_ten_pct = []\n",
    "        for _ in range(np.ceil(len(preds)*.05).astype(np.int16)):\n",
    "            index = np.where(preds == max(preds))[0][0]\n",
    "            top_ten_pct.append(index)\n",
    "            preds[index] = 0.0\n",
    "        next_char = chars_from_ids(np.random.choice(top_ten_pct)+1) \\\n",
    "            .numpy().decode('utf-8')\n",
    "\n",
    "        if ((next_char == '\\n'\n",
    "                or next_char in string.ascii_uppercase\n",
    "                or len(new_word) > name_len) and len(new_word) > 1):\n",
    "            break\n",
    "        else:\n",
    "            # Append the character\n",
    "            new_word += next_char.lower()\n",
    "            # Add pre-padding of zeros to the sequence generated and continue.\n",
    "            seq = ('{0:0>' + str(max_len) + '}').format(new_word).lower()\n",
    "    results.append(new_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "33\n",
      "66\n",
      "99\n",
      "132\n",
      "165\n",
      "198\n",
      "231\n",
      "264\n",
      "297\n",
      "11/11 [==============================] - 1s 4ms/step - loss: 1.4396\n"
     ]
    }
   ],
   "source": [
    "amz_x, amz_y = [], []\n",
    "for word in results:\n",
    "    tmp = (max_len - len(word)) * '0' + word + '\\n'\n",
    "    amz_x.append(tmp[2:])\n",
    "    amz_y.append('')\n",
    "    for i, j in enumerate(word):\n",
    "        if (i >= len(word) - 1):\n",
    "            break\n",
    "        tmp = (max_len - len(word[:-1-i])) * '0' + word[:-1-i].lower()\n",
    "        amz_x.append(tmp)\n",
    "        amz_y.append(word[-1-i])\n",
    "\n",
    "# One Hot Encode the sequences\n",
    "x_amz = np.zeros((len(amz_x), max_len, len(letters)), dtype=bool)\n",
    "y_amz = np.zeros((len(amz_x), len(letters)), dtype=bool)\n",
    "for i, seq in enumerate(amz_x):\n",
    "    for j, c in enumerate(seq):\n",
    "        if c != '0':\n",
    "            x_amz[i, j, ids_from_chars(c).numpy()-1] = 1\n",
    "    y_amz[i, ids_from_chars(amz_y[i]).numpy()-1] = 1\n",
    "    if i % round(.1*x_amz.shape[0]) == 0: print(i)\n",
    "\n",
    "loss = amz_lang_mod.evaluate(x_amz, y_amz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.21912"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp(loss).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using cross_entropy as our loss, we will take tf.exp(loss) to get the perplexity of our model on the result set. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3247f7d4635bb288d9e06d3deacee818856115b2677ccfdf5a578edab993fe5f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
